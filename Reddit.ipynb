{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44936b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Configura le tue credenziali Reddit\n",
    "client_id = 'XpeJWmueFBEGkyXjE-dpcA'\n",
    "client_secret = 'yOyCLHaB0Ur7R0sW75UUmc20MjCPkw'\n",
    "user_agent = 'Samu_Miki'\n",
    "\n",
    "# Funzione per ottenere un token\n",
    "def get_reddit_token(client_id, client_secret, user_agent):\n",
    "    url = 'https://www.reddit.com/api/v1/access_token'\n",
    "    headers = {\n",
    "        'User-Agent': user_agent\n",
    "    }\n",
    "    data = {\n",
    "        'grant_type': 'client_credentials'\n",
    "    }\n",
    "    auth = (client_id, client_secret)\n",
    "    \n",
    "    # Fai la richiesta per ottenere il token\n",
    "    response = requests.post(url, headers=headers, data=data, auth=auth)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        token = response.json()['access_token']\n",
    "        print(f'Token ottenuto con successo: {token}')\n",
    "        return token\n",
    "    else:\n",
    "        print(f'Errore durante l\\'ottenimento del token: {response.status_code}')\n",
    "        return None\n",
    "\n",
    "# Funzione per fare una richiesta all'API Reddit\n",
    "def get_reddit_posts(token):\n",
    "    url = 'https://oauth.reddit.com/r/stocks/top?t=week&limit=100'\n",
    "    headers = {\n",
    "        'Authorization': f'bearer {token}',\n",
    "        'User-Agent': user_agent\n",
    "    }\n",
    "    \n",
    "    # Fai la richiesta per ottenere i post    \n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        \n",
    "        # Estrai i post\n",
    "        posts = data['data']['children']\n",
    "        \n",
    "        # Crea una lista di dizionari con le informazioni che ti interessano\n",
    "        posts_list = []\n",
    "        for post in posts:\n",
    "            post_data = post['data']\n",
    "            posts_list.append({\n",
    "                'id': post_data.get('id'),\n",
    "                'title': post_data.get('title'),\n",
    "                'author': post_data.get('author'),\n",
    "                'score': post_data.get('score'),\n",
    "                'num_comments': post_data.get('num_comments'),\n",
    "                'created_utc': post_data.get('created_utc'),\n",
    "                'permalink': post_data.get('permalink'),\n",
    "                'url': post_data.get('url'),\n",
    "                'subreddit': post_data.get('subreddit')\n",
    "            })\n",
    "\n",
    "        # Crea il DataFrame\n",
    "        df_reddit = pd.DataFrame(posts_list)\n",
    "        df_reddit['created_datetime'] = pd.to_datetime(df_reddit['created_utc'], unit='s')\n",
    "        \n",
    "        print(f'Recuperati {len(df_reddit)} post.')\n",
    "        return df_reddit\n",
    "    \n",
    "    else:\n",
    "        print(f'Errore durante la richiesta ai post: {response.status_code}')\n",
    "        return pd.DataFrame()  # ritorna un df vuoto in caso di errore\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "837e953d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token ottenuto con successo: eyJhbGciOiJSUzI1NiIsImtpZCI6IlNIQTI1NjpzS3dsMnlsV0VtMjVmcXhwTU40cWY4MXE2OWFFdWFyMnpLMUdhVGxjdWNZIiwidHlwIjoiSldUIn0.eyJzdWIiOiJsb2lkIiwiZXhwIjoxNzQ1OTQ1ODI5LjgwNTg1LCJpYXQiOjE3NDU4NTk0MjkuODA1ODUsImp0aSI6IjNpLWYxbDFBSkJSbE9qMXM2Y0tJSkhBVkpIMi1PQSIsImNpZCI6IlhwZUpXbXVlRkJFR2t5WGpFLWRwY0EiLCJsaWQiOiJ0Ml8xbzlueDBlOTBhIiwibGNhIjoxNzQ1ODU5NDI5NzkwLCJzY3AiOiJlSnlLVnRKU2lnVUVBQURfX3dOekFTYyIsImZsbyI6Nn0.MtlITXjR4ffGUtxeubuYfqRJKl_z87MpMxmKiYfN0byo5ICN-8D1JkrXI4U6Pm5BI28RNfDKQARkrdd5zOZWzDI_R3YpYCZhQjChh2HJSMpDJ3_6QRo2MGxCymhDpGQH1FZJlxcbk22YpGq-3RJ1oZaaUmc8BQVj-FvNG40mi20qZGrQBo-y7skAg7b_-DDIjqeWixDDrj_m5gUkunpB0ehSLiS53uOPf5OgO6js6FENkFWFyDqbSaaIHXtrOvEZbygCcF8gNPDbj-M7iQt93hDEptnHPdWjr7n9KiGuC9eODzcBE4WWFY_t4p_x_k25zLCe-4XMi0jC8Vm4GgGjvg\n"
     ]
    }
   ],
   "source": [
    "#ottenere il token:\n",
    "token = get_reddit_token(client_id, client_secret, user_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338e4f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PER ORA NON USARE, SERVE PER AVERNE UNO AL GIORNO, PER INTANTO LO FACCIAMO MANUALMENTE CON I COMANDI SOPRA\n",
    "\n",
    "# Ciclo per ottenere e usare il token ogni giorno\n",
    "while True:\n",
    "    # Ottieni il token (scade dopo 24 ore)\n",
    "    token = get_reddit_token(client_id, client_secret, user_agent)\n",
    "    \n",
    "    if token:\n",
    "        # Usa il token per fare la richiesta all'API\n",
    "        get_reddit_posts(token)\n",
    "    \n",
    "    # Aspetta 24 ore (86400 secondi) prima di ottenere un nuovo token\n",
    "    print('Aspetta 24 ore per ottenere un nuovo token...')\n",
    "    time.sleep(86400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d22b1d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recuperati 100 post.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>score</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>permalink</th>\n",
       "      <th>url</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>created_datetime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1k6pihz</td>\n",
       "      <td>Now we know. It was Retail CEOS who got to Tru...</td>\n",
       "      <td>Epicurus-fan</td>\n",
       "      <td>47308</td>\n",
       "      <td>2923</td>\n",
       "      <td>1.745493e+09</td>\n",
       "      <td>/r/stocks/comments/1k6pihz/now_we_know_it_was_...</td>\n",
       "      <td>https://www.reddit.com/r/stocks/comments/1k6pi...</td>\n",
       "      <td>stocks</td>\n",
       "      <td>2025-04-24 11:09:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1k75t88</td>\n",
       "      <td>Fox Reporter Says the Trump White House Is Giv...</td>\n",
       "      <td>GaussInTheHouse</td>\n",
       "      <td>44876</td>\n",
       "      <td>1059</td>\n",
       "      <td>1.745535e+09</td>\n",
       "      <td>/r/stocks/comments/1k75t88/fox_reporter_says_t...</td>\n",
       "      <td>https://www.reddit.com/r/stocks/comments/1k75t...</td>\n",
       "      <td>stocks</td>\n",
       "      <td>2025-04-24 22:51:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1k5g9yj</td>\n",
       "      <td>Tesla reports disappointing quarterly results ...</td>\n",
       "      <td>Puginator</td>\n",
       "      <td>13937</td>\n",
       "      <td>985</td>\n",
       "      <td>1.745353e+09</td>\n",
       "      <td>/r/stocks/comments/1k5g9yj/tesla_reports_disap...</td>\n",
       "      <td>https://www.reddit.com/r/stocks/comments/1k5g9...</td>\n",
       "      <td>stocks</td>\n",
       "      <td>2025-04-22 20:13:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1k4v51k</td>\n",
       "      <td>WSJ: Dow Headed for Worst April Since 1932 as ...</td>\n",
       "      <td>cambeiu</td>\n",
       "      <td>11031</td>\n",
       "      <td>530</td>\n",
       "      <td>1.745288e+09</td>\n",
       "      <td>/r/stocks/comments/1k4v51k/wsj_dow_headed_for_...</td>\n",
       "      <td>https://www.reddit.com/r/stocks/comments/1k4v5...</td>\n",
       "      <td>stocks</td>\n",
       "      <td>2025-04-22 02:06:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1k5iy1g</td>\n",
       "      <td>Trump says he has ‘no intention’ of firing Fed...</td>\n",
       "      <td>Fidler_2K</td>\n",
       "      <td>8644</td>\n",
       "      <td>894</td>\n",
       "      <td>1.745359e+09</td>\n",
       "      <td>/r/stocks/comments/1k5iy1g/trump_says_he_has_n...</td>\n",
       "      <td>https://www.reddit.com/r/stocks/comments/1k5iy...</td>\n",
       "      <td>stocks</td>\n",
       "      <td>2025-04-22 22:03:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>1k7sc3l</td>\n",
       "      <td>Rolls-Royce Secures £563 Million RAF Typhoon E...</td>\n",
       "      <td>Lofi-Fanboy123</td>\n",
       "      <td>126</td>\n",
       "      <td>9</td>\n",
       "      <td>1.745606e+09</td>\n",
       "      <td>/r/stocks/comments/1k7sc3l/rollsroyce_secures_...</td>\n",
       "      <td>https://www.reddit.com/r/stocks/comments/1k7sc...</td>\n",
       "      <td>stocks</td>\n",
       "      <td>2025-04-25 18:35:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>1k6a1ck</td>\n",
       "      <td>Tesla Robotaxi - Musk sees it as infinite mone...</td>\n",
       "      <td>JuanPabloElTres</td>\n",
       "      <td>124</td>\n",
       "      <td>344</td>\n",
       "      <td>1.745441e+09</td>\n",
       "      <td>/r/stocks/comments/1k6a1ck/tesla_robotaxi_musk...</td>\n",
       "      <td>https://www.reddit.com/r/stocks/comments/1k6a1...</td>\n",
       "      <td>stocks</td>\n",
       "      <td>2025-04-23 20:51:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>1k5xhbh</td>\n",
       "      <td>Bloomberg analysis: Bessent is better than Nav...</td>\n",
       "      <td>TheGoodCod</td>\n",
       "      <td>119</td>\n",
       "      <td>37</td>\n",
       "      <td>1.745410e+09</td>\n",
       "      <td>/r/stocks/comments/1k5xhbh/bloomberg_analysis_...</td>\n",
       "      <td>https://www.reddit.com/r/stocks/comments/1k5xh...</td>\n",
       "      <td>stocks</td>\n",
       "      <td>2025-04-23 12:06:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>1k8t08k</td>\n",
       "      <td>Which of these is not like the others?</td>\n",
       "      <td>30030s</td>\n",
       "      <td>113</td>\n",
       "      <td>59</td>\n",
       "      <td>1.745719e+09</td>\n",
       "      <td>/r/stocks/comments/1k8t08k/which_of_these_is_n...</td>\n",
       "      <td>https://www.reddit.com/r/stocks/comments/1k8t0...</td>\n",
       "      <td>stocks</td>\n",
       "      <td>2025-04-27 01:55:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>1k4umar</td>\n",
       "      <td>It's still too soon to know the impact of tari...</td>\n",
       "      <td>armchairarmadillo</td>\n",
       "      <td>99</td>\n",
       "      <td>27</td>\n",
       "      <td>1.745286e+09</td>\n",
       "      <td>/r/stocks/comments/1k4umar/its_still_too_soon_...</td>\n",
       "      <td>https://www.reddit.com/r/stocks/comments/1k4um...</td>\n",
       "      <td>stocks</td>\n",
       "      <td>2025-04-22 01:40:21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                              title  \\\n",
       "0   1k6pihz  Now we know. It was Retail CEOS who got to Tru...   \n",
       "1   1k75t88  Fox Reporter Says the Trump White House Is Giv...   \n",
       "2   1k5g9yj  Tesla reports disappointing quarterly results ...   \n",
       "3   1k4v51k  WSJ: Dow Headed for Worst April Since 1932 as ...   \n",
       "4   1k5iy1g  Trump says he has ‘no intention’ of firing Fed...   \n",
       "..      ...                                                ...   \n",
       "95  1k7sc3l  Rolls-Royce Secures £563 Million RAF Typhoon E...   \n",
       "96  1k6a1ck  Tesla Robotaxi - Musk sees it as infinite mone...   \n",
       "97  1k5xhbh  Bloomberg analysis: Bessent is better than Nav...   \n",
       "98  1k8t08k             Which of these is not like the others?   \n",
       "99  1k4umar  It's still too soon to know the impact of tari...   \n",
       "\n",
       "               author  score  num_comments   created_utc  \\\n",
       "0        Epicurus-fan  47308          2923  1.745493e+09   \n",
       "1     GaussInTheHouse  44876          1059  1.745535e+09   \n",
       "2           Puginator  13937           985  1.745353e+09   \n",
       "3             cambeiu  11031           530  1.745288e+09   \n",
       "4           Fidler_2K   8644           894  1.745359e+09   \n",
       "..                ...    ...           ...           ...   \n",
       "95     Lofi-Fanboy123    126             9  1.745606e+09   \n",
       "96    JuanPabloElTres    124           344  1.745441e+09   \n",
       "97         TheGoodCod    119            37  1.745410e+09   \n",
       "98             30030s    113            59  1.745719e+09   \n",
       "99  armchairarmadillo     99            27  1.745286e+09   \n",
       "\n",
       "                                            permalink  \\\n",
       "0   /r/stocks/comments/1k6pihz/now_we_know_it_was_...   \n",
       "1   /r/stocks/comments/1k75t88/fox_reporter_says_t...   \n",
       "2   /r/stocks/comments/1k5g9yj/tesla_reports_disap...   \n",
       "3   /r/stocks/comments/1k4v51k/wsj_dow_headed_for_...   \n",
       "4   /r/stocks/comments/1k5iy1g/trump_says_he_has_n...   \n",
       "..                                                ...   \n",
       "95  /r/stocks/comments/1k7sc3l/rollsroyce_secures_...   \n",
       "96  /r/stocks/comments/1k6a1ck/tesla_robotaxi_musk...   \n",
       "97  /r/stocks/comments/1k5xhbh/bloomberg_analysis_...   \n",
       "98  /r/stocks/comments/1k8t08k/which_of_these_is_n...   \n",
       "99  /r/stocks/comments/1k4umar/its_still_too_soon_...   \n",
       "\n",
       "                                                  url subreddit  \\\n",
       "0   https://www.reddit.com/r/stocks/comments/1k6pi...    stocks   \n",
       "1   https://www.reddit.com/r/stocks/comments/1k75t...    stocks   \n",
       "2   https://www.reddit.com/r/stocks/comments/1k5g9...    stocks   \n",
       "3   https://www.reddit.com/r/stocks/comments/1k4v5...    stocks   \n",
       "4   https://www.reddit.com/r/stocks/comments/1k5iy...    stocks   \n",
       "..                                                ...       ...   \n",
       "95  https://www.reddit.com/r/stocks/comments/1k7sc...    stocks   \n",
       "96  https://www.reddit.com/r/stocks/comments/1k6a1...    stocks   \n",
       "97  https://www.reddit.com/r/stocks/comments/1k5xh...    stocks   \n",
       "98  https://www.reddit.com/r/stocks/comments/1k8t0...    stocks   \n",
       "99  https://www.reddit.com/r/stocks/comments/1k4um...    stocks   \n",
       "\n",
       "      created_datetime  \n",
       "0  2025-04-24 11:09:27  \n",
       "1  2025-04-24 22:51:38  \n",
       "2  2025-04-22 20:13:09  \n",
       "3  2025-04-22 02:06:47  \n",
       "4  2025-04-22 22:03:44  \n",
       "..                 ...  \n",
       "95 2025-04-25 18:35:15  \n",
       "96 2025-04-23 20:51:01  \n",
       "97 2025-04-23 12:06:36  \n",
       "98 2025-04-27 01:55:47  \n",
       "99 2025-04-22 01:40:21  \n",
       "\n",
       "[100 rows x 10 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_reddit_posts(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f797355",
   "metadata": {},
   "source": [
    "Il parametro \"top\" ci permette di usare un parametro temporale, ma ne rann davvero pochi (es: se metto hour me ne dà 2), forse conviene usare il parametro \"new\" e continuare a runnarlo ogni tot, e semmai fare uno script per sistemare. es: prende i dati ogni 10 secondi, e ne prende 100, se 10 secondi dopo rirunniamo e 20 post sono uguali, lui si accorge che sono uguali e tiene solo gli altri 80, lo stesso si può fare per l'API di finnhub news che mi dà le news solo giornalmente, per non doverlo runnare solo una volta al giorno. Usando \"After\" si possono prendere veramente molti dati, perchè prende la pagina successiva. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "big-data-finance",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
