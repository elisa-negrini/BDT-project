services:
  zookeeper:
    image: confluentinc/cp-zookeeper:latest
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"

  kafka:
    image: confluentinc/cp-kafka:latest
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
    # volumes:
    #   - ./create-topics.sh:/tmp/create-topics.sh
    # command: ["bash", "/tmp/create-topics.sh"]

  spark-master:
    image: bitnami/spark:latest
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    ports:
      - "7077:7077"
      - "8080:8080"  # Spark master web UI

  spark-worker:
    image: bitnami/spark:latest
    depends_on:
      - spark-master
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=1G
      - SPARK_WORKER_CORES=1
    ports:
      - "8081:8081"  # Spark worker web UI

  # spark-worker-2:
  #   image: bitnami/spark:latest
  #   container_name: spark-worker-2
  #   depends_on:
  #     - spark-master
  #   environment:
  #     - SPARK_MODE=worker
  #     - SPARK_MASTER_URL=spark://spark-master:7077
  #     - SPARK_WORKER_MEMORY=1G
  #     - SPARK_WORKER_CORES=1
  #   ports:
  #     - "8082:8081"  # UI access via http://localhost:8082

  # spark-worker-3:
  #   image: bitnami/spark:latest
  #   container_name: spark-worker-3
  #   depends_on:
  #     - spark-master
  #   environment:
  #     - SPARK_MODE=worker
  #     - SPARK_MASTER_URL=spark://spark-master:7077
  #     - SPARK_WORKER_MEMORY=1G
  #     - SPARK_WORKER_CORES=1
  #   ports:
  #     - "8083:8081"  # UI access via http://localhost:8083

  # spark-worker-4:
  #   image: bitnami/spark:latest
  #   container_name: spark-worker-4
  #   depends_on:
  #     - spark-master
  #   environment:
  #     - SPARK_MODE=worker
  #     - SPARK_MASTER_URL=spark://spark-master:7077
  #     - SPARK_WORKER_MEMORY=1G
  #     - SPARK_WORKER_CORES=1
  #   ports:
  #     - "8084:8081"  # UI access via http://localhost:8083
  


  # Kafka producer data_reddit
  kp_reddit:
    build: ./producer_reddit
    environment:
      KAFKA_SERVER: kafka:9092
    depends_on:
      - kafka
    restart: always
  
  # Kafka producer data_finnhubnews
  kp_finnhubnews:
    build: ./producer_finnhubnews
    environment:
      KAFKA_SERVER: kafka:9092
    depends_on:
      - kafka
    restart: always

  # Kafka producer data_alpaca
  kp_alpaca:
    build: ./producer_alpaca
    environment:
      KAFKA_SERVER: kafka:9092
    depends_on:
      - kafka
    restart: always

  # Kafka producer bluesky
  kp_bluesky:
    build: ./producer_bluesky
    environment:
      KAFKA_SERVER: kafka:9092
    depends_on:
      - kafka
    restart: always
    
  # Kafka producer macrodata
  kp_macrodata:
    build: ./producer_macrodata
    environment:
      KAFKA_SERVER: kafka:9092
    depends_on:
      - kafka
    restart: always

  # Kafka consumer finnhubnews
  kc_finnhubnews:
    build: ./consumer_finnhubnews
    environment:
      KAFKA_SERVER: kafka:9092
      S3_ENDPOINT: http://minio:9000
      S3_ACCESS_KEY: admin
      S3_SECRET_KEY: admin123
      S3_BUCKET: finnhub-data
    depends_on:
      - kafka
      - minio
    volumes:
      - ./consumer_finnhubnews:/app
    working_dir: /app
    command: python consumer_finnhubnews.py
    restart: always

  # # Kc sentiment bluesky
  # kafka-consumer_sentiment:
  #   build: ./sentiment_bluesky
  #   environment:
  #     KAFKA_SERVER: kafka:9092
  #     SOURCE_TOPIC: bluesky
  #     # Optional: Uncomment if you have a target topic
  #     # TARGET_TOPIC: bluesky_sentiment
  #   depends_on:
  #     - kafka
  #   volumes:
  #     - ./sentiment_bluesky:/app
  #   working_dir: /app
  #   command: python sentiment_bluesky.py

  # Kafka consumer alpaca
  kc_alpaca:
    build: ./consumer_alpaca
    depends_on:
      - kafka
      - minio
    environment:
      KAFKA_SERVER: kafka:9092
      S3_ENDPOINT: http://minio:9000
      S3_ACCESS_KEY: admin
      S3_SECRET_KEY: admin123
      S3_BUCKET: stock-data  # Usa il nome del bucket che vuoi utilizzare
    volumes:
      - ./consumer_alpaca:/app
    working_dir: /app
    command: python consumer_alpaca.py
    restart: always

  # Kafka consumer reddit
  kc_reddit:
    build: ./consumer_reddit 
    environment:
      KAFKA_SERVER: kafka:9092
      S3_ENDPOINT: http://minio:9000
      S3_ACCESS_KEY: admin
      S3_SECRET_KEY: admin123
      S3_BUCKET: reddit-data
    depends_on:
      - kafka
      - minio
    volumes:
      - ./consumer_reddit:/app
    working_dir: /app
    command: python consumer_reddit.py
    restart: always

  #kafka consumer bluesky
  kc_bluesky:
    build: ./consumer_bluesky
    depends_on:
      - kafka
      - minio
    environment:
      KAFKA_SERVER: kafka:9092
      S3_ENDPOINT: http://minio:9000
      S3_ACCESS_KEY: admin
      S3_SECRET_KEY: admin123
      S3_BUCKET: bluesky-data
    volumes:
      - ./consumer_bluesky:/app
    working_dir: /app
    command: python consumer_bluesky.py
    restart: always

  #kafka consumer macrodata
  kc_macrodata:
    build: ./consumer_macrodata
    depends_on:
      - kafka
      - minio
    environment:
      KAFKA_SERVER: kafka:9092
      S3_ENDPOINT: http://minio:9000
      S3_ACCESS_KEY: admin
      S3_SECRET_KEY: admin123
      S3_BUCKET: macro-data
    volumes:
      - ./consumer_macrodata:/app
    working_dir: /app
    command: python consumer_macrodata.py
    restart: always


  # # historical macrodata
  # h_macrodata:
  #   build:
  #     context: ./historical_macrodata
  #   container_name: h_macrodata
  #   depends_on:
  #     - spark-master
  #     - minio
  #   environment:
  #     S3_ENDPOINT: http://minio:9000
  #     S3_ACCESS_KEY: admin
  #     S3_SECRET_KEY: admin123
  #     S3_BUCKET: macro-data
  #   networks:
  #     - default

  # # historical alpaca
  # h_alpaca:
  #   build:
  #     context: ./historical_alpaca
  #   container_name: h_alpaca
  #   depends_on:
  #     - spark-master
  #     - minio
  #   environment:
  #     MINIO_ENDPOINT: minio:9000
  #     MINIO_ACCESS_KEY: admin
  #     MINIO_SECRET_KEY: admin123
  #     MINIO_BUCKET: historical-data
  #     ALPACA_API_KEY: PKLJ33WNVLC4SOZQKOB1
  #     ALPACA_SECRET_KEY: ctJiahjzv11ihTZoNRkmOupIzbnwsYkasFMCJekt
  #   restart: "no"

  # historical company
  h_company:
    build:
      context: ./historical_company
    container_name: h_company
    environment:
      MINIO_URL: minio:9000
      MINIO_ACCESS_KEY: admin
      MINIO_SECRET_KEY: admin123
    depends_on:
      - minio

  # Kafka consumer sentiment
  sentiment_bluesky:
    build: ./sentiment_bluesky
    depends_on:
      - kafka
      - spark-master
      - spark-worker
    volumes:
      - ./sentiment_bluesky:/app
    working_dir: /app
    command: >
      spark-submit
      --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0
      sentiment_bluesky.py
    environment:
      - PYSPARK_PYTHON=python3
    restart: always

  # sentiment_reddit:
  #   build: ./sentiment_reddit
  #   depends_on:
  #     - kafka
  #     - spark-master
  #     - spark-worker
  #   volumes:
  #     - ./sentiment_reddit:/app
  #   working_dir: /app
  #   command: >
  #     spark-submit
  #     --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0
  #     sentiment_reddit.py
  #   environment:
  #      - PYSPARK_PYTHON=python3
  #   restart: always


  sentiment_news:
    build: ./sentiment_news
    depends_on:
      - kafka
      - spark-master
      - spark-worker
    volumes:
      - ./sentiment_news:/app
    working_dir: /app
    command: >
      spark-submit
      --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0
      sentiment_news.py
    environment:
       - PYSPARK_PYTHON=python3
    restart: always
    
  


  # flink_preprocessor:
  #   build:
  #     context: ./preprocessing_stream  # <-- cartella dove sta il Dockerfile e preprocess_flink.py
  #     #dockerfile: Dockerfile
  #   container_name: flink_preprocessor
  #   depends_on:
  #     - kafka
  #   #environment:
  #     #- PYFLINK_GATEWAY_DISABLED=true
  #   networks:
  #     - flink_net


  minio:
    image: minio/minio
    container_name: minio
    ports:
     - "9000:9000"   # S3 API
     - "9001:9001"   # MinIO Console (GUI)
    environment:
      MINIO_ROOT_USER: admin
      MINIO_ROOT_PASSWORD: admin123
    volumes:
     - ./data/minio:/data
    command: server /data --console-address ":9001"

  sentiment_consumer:
    build:
      context: ./consumer_sentiment  # <-- Puntamento corretto alla cartella
    depends_on:
      - kafka
    restart: always

networks:
  flink_net:
    driver: bridge

  minio_network:
    driver: bridge



  # spark-app:
  #   build: ./spark-app
  #   depends_on:
  #     - kafka
  #     - spark-master
  #     - spark-worker
  #   environment:
  #     - PYSPARK_PYTHON=python3
  #   volumes:
  #     - ./spark-app:/app
  #   command: >
  #     bash -c "spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.4 /app/app.py"

