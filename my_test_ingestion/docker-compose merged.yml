services:
  zookeeper:
    image: confluentinc/cp-zookeeper:latest
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"

  kafka:
    image: confluentinc/cp-kafka:latest
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
    restart: always
    # volumes:
    #   - ./create-topics.sh:/tmp/create-topics.sh
    # command: ["bash", "/tmp/create-topics.sh"]

  spark-master:
    image: bitnami/spark:latest
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    ports:
      - "7077:7077"
      - "8080:8080"  # Spark master web UI

  spark-worker:
    image: bitnami/spark:latest
    depends_on:
      - spark-master
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=1G
      - SPARK_WORKER_CORES=1
    ports:
      - "8081:8081"  # Spark worker web UI


  minio:
    image: minio/minio
    container_name: minio
    ports:
     - "9000:9000"   # S3 API
     - "9001:9001"   # MinIO Console (GUI)
    environment:
      MINIO_ROOT_USER: admin
      MINIO_ROOT_PASSWORD: admin123
    volumes:
     - ./data/minio:/data
    command: server /data --console-address ":9001"

  # postgre:
  #   image: postgres:16-alpine
  #   container_name: postgre
  #   environment:
  #     POSTGRES_DB: aggregated-data
  #     POSTGRES_USER: admin
  #     POSTGRES_PASSWORD: admin123
  #   ports:
  #     - "5432:5432"
  #   volumes:
  #     - pgdata:/var/lib/postgresql/data
  #   restart: always


### PRODUCER STREAM

  # Kafka producer data_reddit
  kp_reddit:
    build: ./producer_reddit
    environment:
      KAFKA_SERVER: kafka:9092
    depends_on:
      - kafka
    restart: always
  
  #Kafka producer data_finnhubnews
  kp_finnhubnews:
    build: ./producer_finnhubnews
    environment:
      KAFKA_SERVER: kafka:9092
    depends_on:
      - kafka
    restart: always

  # Kafka producer data_alpaca
  # kp_alpaca:
  #   build: ./producer_alpaca
  #   environment:
  #     KAFKA_SERVER: kafka:9092
  #   depends_on:
  #     - kafka
  #   restart: always

  # Kafka producer bluesky
  kp_bluesky:
    build: ./producer_bluesky
    environment:
      KAFKA_SERVER: kafka:9092
    depends_on:
      - kafka
    restart: always
    
  # # Kafka producer macrodata
  # kp_macrodata:
  #   build: ./producer_macrodata
  #   environment:
  #     KAFKA_SERVER: kafka:9092
  #   depends_on:
  #     - kafka
  #   restart: always

# ### CONSUMER STREAM


  # Kafka consumer finnhubnews
  kc_finnhubnews:
    build: ./consumer_finnhubnews
    environment:
      KAFKA_SERVER: kafka:9092
      S3_ENDPOINT: http://minio:9000
      S3_ACCESS_KEY: admin
      S3_SECRET_KEY: admin123
      S3_BUCKET: finnhub-data
    depends_on:
      - kafka
      - minio
    volumes:
      - ./consumer_finnhubnews:/app
    working_dir: /app
    command: python consumer_finnhubnews.py
    restart: always


  # Kafka consumer alpaca
  # kc_alpaca:
  #   build: ./consumer_alpaca
  #   depends_on:
  #     - kafka
  #     - minio
  #   environment:
  #     KAFKA_SERVER: kafka:9092
  #     S3_ENDPOINT: http://minio:9000
  #     S3_ACCESS_KEY: admin
  #     S3_SECRET_KEY: admin123
  #     S3_BUCKET: stock-data
  #   volumes:
  #     - ./consumer_alpaca:/app
  #   working_dir: /app
  #   command: python consumer_alpaca.py
  #   restart: always

  # Kafka consumer reddit
  kc_reddit:
    build: ./consumer_reddit 
    environment:
      KAFKA_SERVER: kafka:9092
      S3_ENDPOINT: http://minio:9000
      S3_ACCESS_KEY: admin
      S3_SECRET_KEY: admin123
      S3_BUCKET: reddit-data
    depends_on:
      - kafka
      - minio
    volumes:
      - ./consumer_reddit:/app
    working_dir: /app
    command: python consumer_reddit.py
    restart: always

  #kafka consumer bluesky
  kc_bluesky:
    build: ./consumer_bluesky
    depends_on:
      - kafka
      - minio
    environment:
      KAFKA_SERVER: kafka:9092
      S3_ENDPOINT: http://minio:9000
      S3_ACCESS_KEY: admin
      S3_SECRET_KEY: admin123
      S3_BUCKET: bluesky-data
    volumes:
      - ./consumer_bluesky:/app
    working_dir: /app
    command: python consumer_bluesky.py
    restart: always

#   #kafka consumer macrodata
#   kc_macrodata:
#     build: ./consumer_macrodata
#     depends_on:
#       - kafka
#       - minio
#     environment:
#       KAFKA_SERVER: kafka:9092
#       S3_ENDPOINT: http://minio:9000
#       S3_ACCESS_KEY: admin
#       S3_SECRET_KEY: admin123
#       S3_BUCKET: macro-data
#     volumes:
#       - ./consumer_macrodata:/app
#     working_dir: /app
#     command: python consumer_macrodata.py
#     restart: always

### SENTIMENT ANALYSIS

  # sentiment_bluesky:
  #   build: ./sentiment_bluesky
  #   depends_on:
  #     - kafka
  #     - spark-master
  #   #  - spark-worker
  #   volumes:
  #     - ./sentiment_bluesky:/app
  #   working_dir: /app
  #   command: >
  #     spark-submit
  #     --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0
  #     sentiment_bluesky.py
  #   environment:
  #     - PYSPARK_PYTHON=python3
  #   restart: always

  sentiment_bluesky:
    build:
      context: .
      dockerfile: ./sentiment_bluesky/Dockerfile
    volumes:
      # Mount per il tuo codice Python.
      - ./sentiment_bluesky:/app
      # Mount per la cartella del modello (tokenizer e .onnx).
      - ./quantized_finbert:/model
    working_dir: /app
    depends_on:
      - kafka
    container_name: sentiment_bluesky
    restart: always
    environment:
      # *** QUESTA È LA MODIFICA CHIAVE ***
      # Dice a PyFlink di includere i JAR dalle directory standard di Flink
      # (/opt/flink/lib e /opt/flink/opt) nella classpath.
      # Il connettore Kafka verrà copiato in /opt/flink/lib dal Dockerfile.
      - PYFLINK_CLASSPATH=/opt/flink/lib/*:/opt/flink/opt/*  
      - FINBERT_MODEL_BASE_PATH=/model 
    mem_limit: 4g # Consider increasing this if you raise parallelism
    cpus: 2 
    


#   sentiment_reddit:
#     build: ./sentiment_reddit
#     depends_on:
#       - kafka
#       - spark-master
#       - spark-worker
#     volumes:
#       - ./sentiment_reddit:/app
#     working_dir: /app
#     command: >
#       spark-submit
#       --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0
#       sentiment_reddit.py
#     environment:
#        - PYSPARK_PYTHON=python3
#     restart: always



  sentiment_reddit:
    build:
      context: .
      dockerfile: ./sentiment_reddit/Dockerfile
    volumes:
      # Mount per il tuo codice Python.
      - ./sentiment_reddit:/app
      # Mount per la cartella del modello (tokenizer e .onnx).
      - ./quantized_finbert:/model
    working_dir: /app
    depends_on:
      - kafka
    container_name: sentiment_reddit
    restart: always
    environment:
      # *** QUESTA È LA MODIFICA CHIAVE ***
      # Dice a PyFlink di includere i JAR dalle directory standard di Flink
      # (/opt/flink/lib e /opt/flink/opt) nella classpath.
      # Il connettore Kafka verrà copiato in /opt/flink/lib dal Dockerfile.
      - PYFLINK_CLASSPATH=/opt/flink/lib/*:/opt/flink/opt/*  
      - FINBERT_MODEL_BASE_PATH=/model 
    mem_limit: 4g # Consider increasing this if you raise parallelism
    cpus: 2





  # sentiment_news:
  #   build: ./sentiment_news
  #   depends_on:
  #     - kafka
  #     - spark-master
  #   #  - spark-worker
  #   volumes:
  #     - ./sentiment_news:/app
  #   working_dir: /app
  #   command: >
  #     spark-submit
  #     --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0
  #     sentiment_news.py
  #   environment:
  #      - PYSPARK_PYTHON=python3
  #   restart: always
    

  sentiment_news:
    build:
      context: .
      dockerfile: ./sentiment_news/Dockerfile
    volumes:
      # Mount per il tuo codice Python.
      - ./sentiment_news:/app
      # Mount per la cartella del modello (tokenizer e .onnx).
      - ./quantized_finbert:/model
    working_dir: /app
    depends_on:
      - kafka
    container_name: sentiment_news
    restart: always
    environment:
      # *** QUESTA È LA MODIFICA CHIAVE ***
      # Dice a PyFlink di includere i JAR dalle directory standard di Flink
      # (/opt/flink/lib e /opt/flink/opt) nella classpath.
      # Il connettore Kafka verrà copiato in /opt/flink/lib dal Dockerfile.
      - PYFLINK_CLASSPATH=/opt/flink/lib/*:/opt/flink/opt/*  
      - FINBERT_MODEL_BASE_PATH=/model 
    mem_limit: 4g # Consider increasing this if you raise parallelism
    cpus: 2 






  
  sentiment_consumer:
    build:
      context: ./consumer_sentiment
    depends_on:
      - kafka
      - minio  # <-- Assicurati che ci sia un servizio "minio" definito
    environment:
      S3_ENDPOINT: http://minio:9000
      S3_ACCESS_KEY: admin
      S3_SECRET_KEY: admin123
      S3_BUCKET: sentiment-data
    networks:
      - default  # o il nome della rete Docker se ne hai definita una custom
    restart: always

# ### AGGREGATION LAYER

  # flink:
  #   build:
  #     context: ./preprocessing_stream
  #   container_name: flink
  #   depends_on: 
  #     - kafka
  #   restart: always

# ### PREDICTION LAYER

#   predictor_layer:
#     build:
#       context: ./prediction_layer 
#     depends_on:
#       - kafka
#     restart: always

#   predictor_layer2:
#     build:
#       context: ./prediction_layer2 
#     depends_on:
#       - kafka
#     environment:
#       - BOOTSTRAP_SERVERS: kafka:9092
#     restart: always


# ### DASHBOARD 

#   dashboard:
#     build:
#       context: ./dashboard
#       dockerfile: Dockerfile
#     container_name: dashboard
#     ports:
#       - "8501:8501"
#     depends_on:
#       - kafka
#     environment:
#       - PYTHONUNBUFFERED=1

# ## CLEANER MINIO

  # clear-minio:
  #   build:
  #     context: ./clean_minio
  #   environment:
  #     MINIO_URL: "minio:9000"
  #     MINIO_ACCESS_KEY: "admin"
  #     MINIO_SECRET_KEY: "admin123"
  #   command: ["python", "clean_minio.py"]


# HISTORICAL CONTAINER

# Historical producer

  # kp_h_macrodata:
  #   build: ./kp_historical_macrodata
  #   environment:
  #     KAFKA_SERVER: kafka:9092
  #   depends_on:
  #     - kafka

  # kp_h_alpaca:
  #   build: ./kp_historical_alpaca
  #   environment:
  #     KAFKA_SERVER: kafka:9092
  #   depends_on:
  #     - kafka
  #   restart: always

  # kp_h_company:
  #   build:
  #     context: ./kp_historical_company
  #   container_name: kp_h_company
  #   environment:
  #     KAFKA_SERVER: kafka:9092
  #   depends_on:
  #     - kafka

# historical consumer
  # kc_h_company:
  #   build:
  #     context: ./kc_historical_company
  #   container_name: kc_h_company
  #   depends_on:
  #     - kafka
  #     - minio
  #   environment:
  #     S3_ENDPOINT: http://minio:9000
  #     S3_ACCESS_KEY: admin
  #     S3_SECRET_KEY: admin123
      
  # kc_h_alpaca:
  #   build:
  #     context: ./kc_historical_alpaca
  #   container_name: kc_h_alpaca
  #   depends_on:
  #     - kafka
  #     - spark-master
  #     - minio
  #   environment:
  #     S3_ENDPOINT: http://minio:9000
  #     S3_ACCESS_KEY: admin
  #     S3_SECRET_KEY: admin123
  #     S3_BUCKET: historical-data

  # kc_h_macrodata:
  #   build:
  #     context: ./kc_historical_macrodata
  #   container_name: kc_h_macrodata
  #   depends_on:
  #     - spark-master
  #     - kafka
  #     - minio
  #   environment:
  #     S3_ENDPOINT: http://minio:9000
  #     S3_ACCESS_KEY: admin
  #     S3_SECRET_KEY: admin123
  #     S3_BUCKET: macro-data
  #     SPARK_SUBMIT_OPTIONS: --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0

  # flink_historical:
  #     build:
  #       context: ./historical_aggregated
  #     container_name: flink_historical
  #     depends_on:
  #       - kafka
  #       - postgre
  #     restart: always
  #     environment:
  #       - KAFKA_BOOTSTRAP_SERVERS=kafka:9092
  #       - POSTGRES_HOST=postgre
  #       - POSTGRES_PORT=5432
  #       - POSTGRES_DB=aggregated-data
  #       - POSTGRES_USER=admin
  #       - POSTGRES_PASSWORD=admin123


networks:
  flink_net:
    driver: bridge

  minio_network:
    driver: bridge


volumes:
  pgdata:
    driver: local 