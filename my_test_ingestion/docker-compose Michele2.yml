
services:
  zookeeper:
    image: confluentinc/cp-zookeeper:latest
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"

  kafka:
    image: confluentinc/cp-kafka:latest
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
    # volumes:
    #   - ./create-topics.sh:/tmp/create-topics.sh
    # command: ["bash", "/tmp/create-topics.sh"]

  spark-master:
    image: bitnami/spark:latest
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    ports:
      - "7077:7077"
      - "8080:8080"  # Spark master web UI

  spark-worker:
    image: bitnami/spark:latest
    depends_on:
      - spark-master
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=1G
      - SPARK_WORKER_CORES=1
    ports:
      - "8081:8081"  # Spark worker web UI
    
  # spark-worker-2:
  #   image: bitnami/spark:latest
  #   container_name: spark-worker-2
  #   depends_on:
  #     - spark-master
  #   environment:
  #     - SPARK_MODE=worker
  #     - SPARK_MASTER_URL=spark://spark-master:7077
  #     - SPARK_WORKER_MEMORY=1G
  #     - SPARK_WORKER_CORES=1
  #   ports:
  #     - "8082:8081"   # Worker 2 UI (host:container)

  # spark-worker-3:
  #   image: bitnami/spark:latest
  #   container_name: spark-worker-3
  #   depends_on:
  #     - spark-master
  #   environment:
  #     - SPARK_MODE=worker
  #     - SPARK_MASTER_URL=spark://spark-master:7077
  #     - SPARK_WORKER_MEMORY=1G
  #     - SPARK_WORKER_CORES=1
  #   ports:
  #     - "8083:8081"   # Worker 3 UI (host:container)


  spark-writer:
    build: ./spark-writer
    depends_on:
      - spark-master
      - spark-worker
      - kafka
      - minio
    environment:
      AWS_ACCESS_KEY_ID: ${MINIO_ACCESS_KEY}
      AWS_SECRET_ACCESS_KEY: ${MINIO_SECRET_KEY}
      API_KEY_ALPACA: ${API_KEY_ALPACA}
      API_SECRET_ALPACA: ${API_SECRET_ALPACA}
      API_KEY_MACRO: ${API_KEY_MACRO}
      BASE_URL_MACRO: ${BASE_URL_MACRO}
      SPARK_PROPERTIES: |
        {
          "spark.hadoop.fs.s3a.endpoint": "${MINIO_ENDPOINT}",
          "spark.hadoop.fs.s3a.path.style.access": "true",
          "spark.hadoop.fs.s3a.impl": "org.apache.hadoop.fs.s3a.S3AFileSystem"
        }
    volumes:
      - ./spark-writer:/app
    command: >
      /opt/bitnami/spark/bin/spark-submit --master spark://spark-master:7077 /app/historical_data_writer.py



  # Kafka producer data_reddit
  kp_reddit:
    build: ./producer_reddit
    environment:
      KAFKA_SERVER: ${KAFKA_SERVER}
      REDDIT_CLIENT_ID: ${REDDIT_CLIENT_ID}
      REDDIT_CLIENT_SECRET: ${REDDIT_CLIENT_SECRET}
      REDDIT_USER_AGENT: ${REDDIT_USER_AGENT}
    depends_on:
      - kafka

  
  # Kafka producer data_finnhubnews
  kp_finnhubnews:
    build: ./producer_finnhubnews
    environment:
      KAFKA_SERVER: ${KAFKA_SERVER}
      API_KEY_FINNHUB: ${API_KEY_FINNHUB}
    depends_on:
      - kafka

  # Kafka producer data_alpaca
  kp_alpaca:
    build: ./producer_alpaca
    environment:
      KAFKA_SERVER: ${KAFKA_SERVER}
      API_KEY_ALPACA: ${API_KEY_ALPACA}
      API_SECRET_ALPACA: ${API_SECRET_ALPACA}
    depends_on:
      - kafka

  # Kafka producer bluesky
  kp_bluesky:
    build: ./producer_bluesky
    environment:
      IDENTIFIER: ${IDENTIFIER}
      PASSWORD: ${PASSWORD}
      BLUESKY_URL: ${BLUESKY_URL}
      KAFKA_SERVER: ${KAFKA_SERVER}
    depends_on:
      - kafka
    
  # Kafka producer macrodata
  kp_macrodata:
    build: ./producer_macrodata
    environment:
      KAFKA_SERVER: ${KAFKA_SERVER}
      API_KEY_MACRO: ${API_KEY_MACRO}
      BASE_URL_MACRO: ${BASE_URL_MACRO}
    depends_on:
      - kafka

  # Kafka consumer finnhubnews
  kc_finnhubnews:
    build: ./consumer_finnhubnews
    environment:
      KAFKA_SERVER: ${KAFKA_SERVER}
      S3_ENDPOINT: ${MINIO_ENDPOINT}
      S3_ACCESS_KEY: ${MINIO_ACCESS_KEY}
      S3_SECRET_KEY: ${MINIO_SECRET_KEY}
    depends_on:
      - kafka
      - minio
    volumes:
      - ./consumer_finnhubnews:/app
    working_dir: /app
    command: python consumer_finnhubnews.py

  # Kc sentiment bluesky
  kafka-consumer_sentiment:
    build: ./sentiment_bluesky
    environment:
      KAFKA_SERVER: ${KAFKA_SERVER}
      # Optional: Uncomment if you have a target topic
      # TARGET_TOPIC: bluesky_sentiment
    depends_on:
      - kafka
    volumes:
      - ./sentiment_bluesky:/app
    working_dir: /app
    command: python sentiment_bluesky.py

  # Kafka consumer alpaca
  kc_alpaca:
    build: ./consumer_alpaca
    depends_on:
      - kafka
      - minio
    environment:
      KAFKA_SERVER: ${KAFKA_SERVER}
      S3_ENDPOINT: ${MINIO_ENDPOINT}
      S3_ACCESS_KEY: ${MINIO_ACCESS_KEY}
      S3_SECRET_KEY: ${MINIO_SECRET_KEY}
      S3_BUCKET: stock-data  # Usa il nome del bucket che vuoi utilizzare
    volumes:
      - ./consumer_alpaca:/app
    working_dir: /app
    command: python consumer_alpaca.py

  # Kafka consumer reddit
  kc_reddit:
    build: ./consumer_reddit 
    environment:
      KAFKA_SERVER: ${KAFKA_SERVER}
      S3_ENDPOINT: ${MINIO_ENDPOINT}
      S3_ACCESS_KEY: ${MINIO_ACCESS_KEY}
      S3_SECRET_KEY: ${MINIO_SECRET_KEY}
      S3_BUCKET: reddit-data
    depends_on:
      - kafka
      - minio
    volumes:
      - ./consumer_reddit:/app
    working_dir: /app
    command: python consumer_reddit.py

  #kafka consumer bluesky
  kc_bluesky:
    build: ./consumer_bluesky
    depends_on:
      - kafka
      - minio
    environment:
      KAFKA_SERVER: ${KAFKA_SERVER}
      S3_ENDPOINT: ${MINIO_ENDPOINT}
      S3_ACCESS_KEY: ${MINIO_ACCESS_KEY}
      S3_SECRET_KEY: ${MINIO_SECRET_KEY}
      S3_BUCKET: bluesky-data
    volumes:
      - ./consumer_bluesky:/app
    working_dir: /app
    command: python consumer_bluesky.py

  #kafka consumer macrodata
  kc_macrodata:
    build: ./consumer_macrodata
    depends_on:
      - kafka
      - minio
    environment:
      KAFKA_SERVER: ${KAFKA_SERVER}
      S3_ENDPOINT: ${MINIO_ENDPOINT}
      S3_ACCESS_KEY: ${MINIO_ACCESS_KEY}
      S3_SECRET_KEY: ${MINIO_SECRET_KEY}
      S3_BUCKET: macro-data
    volumes:
      - ./consumer_macrodata:/app
    working_dir: /app
    command: python consumer_macrodata.py

  # historical alpaca
  h_alpaca:
    build: ./historical_alpaca/
    container_name: h_alpaca
    depends_on:
      - minio
    environment:
      ALPACA_API_KEY: ${API_KEY_ALPACA}
      ALPACA_SECRET_KEY: ${API_SECRET_ALPACA}
      S3_ENDPOINT: ${MINIO_ENDPOINT}
      S3_ACCESS_KEY: ${MINIO_ACCESS_KEY}
      S3_SECRET_KEY: ${MINIO_SECRET_KEY}
      S3_BUCKET: historical_alpaca
    volumes:
      - ./historical_alpaca:/app
    networks:
      - minio_network
    restart: "no"

  # historical data stocks 
  h_data:
    build: ./historical_data/
    container_name: h_data
    depends_on:
      - minio
    environment:
      S3_ENDPOINT: ${MINIO_ENDPOINT}
      S3_ACCESS_KEY: ${MINIO_ACCESS_KEY}
      S3_SECRET_KEY: ${MINIO_SECRET_KEY}
      S3_BUCKET: historical_data
    volumes:
      - ./historical_data:/app
    working_dir: /app
    command: spark-submit historical_alpaca.py
    networks:
      - minio_network
    restart: "no"

  # historical macrodata spark
  h_macrodata:
    build:
      context: ./historical_macrodata
    container_name: h_macrodata
    depends_on:
      - spark-master
      - minio
    environment:
      S3_ENDPOINT: ${MINIO_ENDPOINT}
      S3_ACCESS_KEY: ${MINIO_ACCESS_KEY}
      S3_SECRET_KEY: ${MINIO_SECRET_KEY}
      S3_BUCKET: macro-data
    networks:
      - default

  # historical company
  h_company:
    build:
      context: ./historical_company
    container_name: h_company
    environment:
      MINIO_URL: ${MINIO_ENDPOINT}
      MINIO_ACCESS_KEY: ${MINIO_ACCESS_KEY}
      MINIO_SECRET_KEY: ${MINIO_SECRET_KEY}
    depends_on:
      - minio

  minio:
    image: minio/minio
    container_name: minio
    ports:
     - "9000:9000"   # S3 API
     - "9001:9001"   # MinIO Console (GUI)
    environment:
      MINIO_ROOT_USER: ${MINIO_ACCESS_KEY}
      MINIO_ROOT_PASSWORD: ${MINIO_SECRET_KEY}
    volumes:
     - ./data/minio:/data
    command: server /data --console-address ":9001"



networks:
  minio_network:
    driver: bridge


  # spark-app:
  #   build: ./spark-app
  #   depends_on:
  #     - kafka
  #     - spark-master
  #     - spark-worker
  #   environment:
  #     - PYSPARK_PYTHON=python3
  #   volumes:
  #     - ./spark-app:/app
  #   command: >
  #     bash -c "spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.4 /app/app.py"

