services:
  zookeeper:
    image: confluentinc/cp-zookeeper:latest
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"

  kafka:
    image: confluentinc/cp-kafka:latest
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
    # volumes:
    #   - ./create-topics.sh:/tmp/create-topics.sh
    # command: ["bash", "/tmp/create-topics.sh"]

  spark-master:
    image: bitnami/spark:latest
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    ports:
      - "7077:7077"
      - "8080:8080"  # Spark master web UI

  spark-worker:
    image: bitnami/spark:latest
    depends_on:
      - spark-master
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=1G
      - SPARK_WORKER_CORES=1
    ports:
      - "8081:8081"  # Spark worker web UI
    
  # spark-writer:
  #   build: ./spark-writer
  #   depends_on:
  #      - spark-master
  #      - spark-worker
  #      - kafka
  #      - minio
  #   environment:
  #     AWS_ACCESS_KEY_ID: admin
  #     AWS_SECRET_ACCESS_KEY: admin123
  #     SPARK_PROPERTIES: |
  #       {
  #         "spark.hadoop.fs.s3a.endpoint":"http://minio:9000",
  #         "spark.hadoop.fs.s3a.path.style.access":"true",
  #         "spark.hadoop.fs.s3a.impl":"org.apache.hadoop.fs.s3a.S3AFileSystem"
  #       }
  #   volumes:
  #     - ./spark-writer:/app
  #   command: >
  #     /opt/spark/bin/spark-submit --master spark://spark-master:7077 /app/spark_writer.py



  flink_preprocessor:
    build:
      context: ./preprocessing_stream  # <-- cartella dove sta il Dockerfile e preprocess_flink.py
      #dockerfile: Dockerfile
    container_name: flink_preprocessor
    depends_on:
      - kafka
    #environment:
      #- PYFLINK_GATEWAY_DISABLED=true
    networks:
      - flink_net

  # predictor_consumer:
  #   build:
  #     context: .
  #     dockerfile: Dockerfile
  #   depends_on:
  #     - broker
  #   environment:
  #     KAFKA_BROKER: broker:9092
  #   volumes:
  #     - ./ml_modeling/models_multivariate:/app/ml_modeling/models_multivariate


  # Kafka producer data_reddit
  kp_reddit:
    build: ./producer_reddit
    environment:
      KAFKA_SERVER: kafka:9092
    depends_on:
      - kafka
  
  # Kafka producer data_finnhubnews
  kp_finnhubnews:
    build: ./producer_finnhubnews
    environment:
      KAFKA_SERVER: kafka:9092
    depends_on:
      - kafka

  # Kafka producer data_alpaca
  kp_alpaca:
    build: ./producer_alpaca
    environment:
      KAFKA_SERVER: kafka:9092
    depends_on:
      - kafka

  # Kafka producer bluesky
  kp_bluesky:
    build: ./producer_bluesky
    environment:
      KAFKA_SERVER: kafka:9092
    depends_on:
      - kafka
    
  # Kafka producer macrodata
  kp_macrodata:
    build: ./producer_macrodata
    environment:
      KAFKA_SERVER: kafka:9092
    depends_on:
      - kafka

  # Kafka consumer finnhubnews
  kc_finnhubnews:
    build: ./consumer_finnhubnews
    environment:
      KAFKA_SERVER: kafka:9092
      S3_ENDPOINT: http://minio:9000
      S3_ACCESS_KEY: admin
      S3_SECRET_KEY: admin123
      S3_BUCKET: finnhub-data
    depends_on:
      - kafka
      - minio
    volumes:
      - ./consumer_finnhubnews:/app
    working_dir: /app
    command: python consumer_finnhubnews.py

  # # Kafka consumer sentiment
  # kafka-consumer_sentiment:
  #   build: ./sentiment_bluesky
  #   environment:
  #     KAFKA_SERVER: kafka:9092
  #     SOURCE_TOPIC: bluesky
  #     # Optional: Uncomment if you have a target topic
  #     # TARGET_TOPIC: bluesky_sentiment
  #   depends_on:
  #     - kafka
  #   volumes:
  #     - ./sentiment_bluesky:/app
  #   working_dir: /app
  #   command: python sentiment_bluesky.py

  # Kafka consumer alpaca
  kc_alpaca:
    build: ./consumer_alpaca
    depends_on:
      - kafka
      - minio
    environment:
      KAFKA_SERVER: kafka:9092
      S3_ENDPOINT: http://minio:9000
      S3_ACCESS_KEY: admin
      S3_SECRET_KEY: admin123
      S3_BUCKET: stock-data  # Usa il nome del bucket che vuoi utilizzare
    volumes:
      - ./consumer_alpaca:/app
    working_dir: /app
    command: python consumer_alpaca.py

  # Kafka consumer reddit
  kc_reddit:
    build: ./consumer_reddit 
    environment:
      KAFKA_SERVER: kafka:9092
      S3_ENDPOINT: http://minio:9000
      S3_ACCESS_KEY: admin
      S3_SECRET_KEY: admin123
      S3_BUCKET: reddit-data
    depends_on:
      - kafka
      - minio
    volumes:
      - ./consumer_reddit:/app
    working_dir: /app
    command: python consumer_reddit.py

  #kafka consumer bluesky
  kc_bluesky:
    build: ./consumer_bluesky
    depends_on:
      - kafka
      - minio
    environment:
      KAFKA_SERVER: kafka:9092
      S3_ENDPOINT: http://minio:9000
      S3_ACCESS_KEY: admin
      S3_SECRET_KEY: admin123
      S3_BUCKET: bluesky-data
    volumes:
      - ./consumer_bluesky:/app
    working_dir: /app
    command: python consumer_bluesky.py

  #kafka consumer macrodata
  kc_macrodata:
    build: ./consumer_macrodata
    depends_on:
      - kafka
      - minio
    environment:
      KAFKA_SERVER: kafka:9092
      S3_ENDPOINT: http://minio:9000
      S3_ACCESS_KEY: admin
      S3_SECRET_KEY: admin123
      S3_BUCKET: macro-data
    volumes:
      - ./consumer_macrodata:/app
    working_dir: /app
    command: python consumer_macrodata.py

  # historical alpaca
  historical_alpaca:
    build: ./historical_alpaca/
    container_name: historical_alpaca
    depends_on:
      - minio
    environment:
      ALPACA_API_KEY: PKCAB7RKX3K5IH490XM6
      ALPACA_SECRET_KEY: j0t7VFg1B1Ew2vzUIPe4GssT7rAkmvxXpczi5htP
      S3_ENDPOINT: http://minio:9000
      S3_ACCESS_KEY: admin
      S3_SECRET_KEY: admin123
      S3_BUCKET: historical_alpaca
    volumes:
      - ./historical_alpaca:/app
    networks:
      - minio_network
    restart: "no"

  # historical macrodata
  historical-macrodata:
    build: ./historical_macrodata/
    container_name: historical_macrodata
    depends_on:
      - minio
    environment:
      S3_ENDPOINT: http://minio:9000
      S3_ACCESS_KEY: admin
      S3_SECRET_KEY: admin123
      S3_BUCKET: macro-data
    volumes:
      - ./historical_macrodata:/app
    networks:
      - minio_network
    restart: "no" 

  minio:
    image: minio/minio
    container_name: minio
    ports:
     - "9000:9000"   # S3 API
     - "9001:9001"   # MinIO Console (GUI)
    environment:
      MINIO_ROOT_USER: admin
      MINIO_ROOT_PASSWORD: admin123
    volumes:
     - ./data/minio:/data
    command: server /data --console-address ":9001"

# Nel fondo del file docker-compose.yml, unifica le reti:
networks:
  flink_net:
    driver: bridge

  minio_network:
    driver: bridge



  # spark-app:
  #   build: ./spark-app
  #   depends_on:
  #     - kafka
  #     - spark-master
  #     - spark-worker
  #   environment:
  #     - PYSPARK_PYTHON=python3
  #   volumes:
  #     - ./spark-app:/app
  #   command: >
  #     bash -c "spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.4 /app/app.py"

