services:
  zookeeper:
    image: confluentinc/cp-zookeeper:latest
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"

  kafka:
    image: confluentinc/cp-kafka:latest
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
    environment:
          KAFKA_BROKER_ID: 1
          KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
          KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
          KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
    restart: always
    # volumes:
    #   - ./create-topics.sh:/tmp/create-topics.sh
    # command: ["bash", "/tmp/create-topics.sh"]


  minio:
    image: minio/minio
    container_name: minio
    ports:
     - "9000:9000"
     - "9001:9001"
    environment:
      MINIO_ROOT_USER: ${S3_ACCESS_KEY} 
      MINIO_ROOT_PASSWORD: ${S3_SECRET_KEY}
    volumes:
     - ./data/minio:/data
    command: server /data --console-address ":9001"

  # postgre:
  #   image: postgres:16-alpine
  #   container_name: postgre
  #   environment:
  #     POSTGRES_DB: ${DB_NAME}
  #     POSTGRES_USER: ${DB_USER}
  #     POSTGRES_PASSWORD: ${DB_PASSWORD}
  #   ports:
  #     - "5432:5432"
  #   volumes:
  #     - pgdata:/var/lib/postgresql/data
  #   restart: always

  postgre:
      build:
        context: ./postgresql
        dockerfile: Dockerfile
      environment:
        POSTGRES_DB: ${DB_NAME}
        POSTGRES_USER: ${DB_USER}
        POSTGRES_PASSWORD: ${DB_PASSWORD}
      ports:
        - "5432:5432"
      volumes:
        - pgdata:/var/lib/postgresql/data
      restart: always

  # db:
  #   build:
  #     context: ./postgresql
  #   restart: no
  #   shm_size: 128mb
  #   environment:
  #     POSTGRES_PASSWORD: ${DB_PASSWORD}
  #     POSTGRES_DB: ${DB_NAME}
  #   ports:
  #     - "5433:5432"


### PRODUCER STREAM

  # Kafka producer data_reddit
  # kp_reddit:
  #   build: ./producer_reddit
  #   environment:
  #     REDDIT_CLIENT_ID: ${REDDIT_CLIENT_ID}
  #     REDDIT_CLIENT_SECRET: ${REDDIT_CLIENT_SECRET}
  #     REDDIT_USER_AGENT: ${REDDIT_USER_AGENT}
  #     KAFKA_BOOTSTRAP_SERVERS: ${KAFKA_BOOTSTRAP_SERVERS}
  #   depends_on:
  #     - kafka
  #   restart: always
  
  # Kafka producer data_finnhubnews
  kp_finnhubnews:
    build: ./producer_finnhubnews
    environment:
      KAFKA_BOOTSTRAP_SERVERS: ${KAFKA_BOOTSTRAP_SERVERS}
      FINNHUB_API_KEY: ${FINNHUB_API_KEY}
    depends_on:
      - kafka
    restart: always

 # Kafka producer data_alpaca
  kp_alpaca:
    build: ./producer_alpaca
    environment:
      API_KEY_ALPACA: ${API_KEY_ALPACA}
      API_SECRET_ALPACA: ${API_SECRET_ALPACA}
      KAFKA_BOOTSTRAP_SERVERS: ${KAFKA_BOOTSTRAP_SERVERS}
      POSTGRES_HOST: ${DB_HOST} 
      POSTGRES_PORT: ${DB_PORT} 
      POSTGRES_DB: ${DB_NAME}   
      POSTGRES_USER: ${DB_USER} 
      POSTGRES_PASSWORD: ${DB_PASSWORD} 
    depends_on:
      - kafka
    restart: always

  # Kafka producer bluesky
  kp_bluesky:
    build: ./producer_bluesky
    environment:
      BLUESKY_IDENTIFIER: ${BLUESKY_IDENTIFIER}
      BLUESKY_PASSWORD: ${BLUESKY_PASSWORD}
      KAFKA_BOOTSTRAP_SERVERS: ${KAFKA_BOOTSTRAP_SERVERS}
    depends_on:
      - kafka
    restart: always
    
  # Kafka producer macrodata
  kp_macrodata:
    build: ./producer_macrodata
    environment:
      API_KEY_FRED: ${API_KEY_FRED}
      KAFKA_BOOTSTRAP_SERVERS: ${KAFKA_BOOTSTRAP_SERVERS}
    depends_on:
      - kafka
    restart: always

# ### CONSUMER STREAM

  # Kafka consumer finnhubnews
  # kc_finnhubnews:
  #   build: ./consumer_finnhubnews
  #   environment:
  #     KAFKA_BOOTSTRAP_SERVERS: ${KAFKA_BOOTSTRAP_SERVERS}
  #     S3_ENDPOINT_URL: ${S3_ENDPOINT_URL} 
  #     S3_ACCESS_KEY: ${S3_ACCESS_KEY}
  #     S3_SECRET_KEY: ${S3_SECRET_KEY}
  #   depends_on:
  #     - kafka
  #     - minio
  #   volumes:
  #     - ./consumer_finnhubnews:/app
  #   working_dir: /app
  #   command: python consumer_finnhubnews.py
  #   restart: always


  # Kafka consumer alpaca
  # kc_alpaca:
  #   build: ./consumer_alpaca
  #   depends_on:
  #     - kafka
  #     - minio
  #   environment:
  #     KAFKA_BOOTSTRAP_SERVERS: ${KAFKA_BOOTSTRAP_SERVERS}
  #     S3_ENDPOINT_URL: ${S3_ENDPOINT_URL} 
  #     S3_ACCESS_KEY: ${S3_ACCESS_KEY}
  #     S3_SECRET_KEY: ${S3_SECRET_KEY}
  #   volumes:
  #     - ./consumer_alpaca:/app
  #   working_dir: /app
  #   command: python consumer_alpaca.py
  #   restart: always

  # Kafka consumer reddit
  # kc_reddit:
  #   build: ./consumer_reddit 
  #   environment:
  #     KAFKA_BOOTSTRAP_SERVERS: ${KAFKA_BOOTSTRAP_SERVERS}
  #     S3_ENDPOINT_URL: ${S3_ENDPOINT_URL} 
  #     S3_ACCESS_KEY: ${S3_ACCESS_KEY}
  #     S3_SECRET_KEY: ${S3_SECRET_KEY}
  #   depends_on:
  #     - kafka
  #     - minio
  #   volumes:
  #     - ./consumer_reddit:/app
  #   working_dir: /app
  #   command: python consumer_reddit.py
  #   restart: always

  #kafka consumer bluesky
  # kc_bluesky:
  #   build: ./consumer_bluesky
  #   depends_on:
  #     - kafka
  #     - minio
  #   environment:
  #     KAFKA_BOOTSTRAP_SERVERS: ${KAFKA_BOOTSTRAP_SERVERS}
  #     S3_ENDPOINT_URL: ${S3_ENDPOINT_URL} 
  #     S3_ACCESS_KEY: ${S3_ACCESS_KEY}
  #     S3_SECRET_KEY: ${S3_SECRET_KEY}
  #   volumes:
  #     - ./consumer_bluesky:/app
  #   working_dir: /app
  #   command: python consumer_bluesky.py
  #   restart: always

  #kafka consumer macrodata
  # kc_macrodata:
  #   build: ./consumer_macrodata
  #   depends_on:
  #     - kafka
  #     - minio
  #   environment:
  #     KAFKA_BOOTSTRAP_SERVERS: ${KAFKA_BOOTSTRAP_SERVERS}
  #     S3_ENDPOINT_URL: ${S3_ENDPOINT_URL} 
  #     S3_ACCESS_KEY: ${S3_ACCESS_KEY}
  #     S3_SECRET_KEY: ${S3_SECRET_KEY}
  #   volumes:
  #     - ./consumer_macrodata:/app
  #   working_dir: /app
  #   command: python consumer_macrodata.py
  #   restart: always

## SENTIMENT ANALYSIS

  sentiment_bluesky:
    build:
      context: .
      dockerfile: ./sentiment_bluesky/Dockerfile
    volumes:
      # Mount per il tuo codice Python.
      - ./sentiment_bluesky:/app
      # Mount per la cartella del modello (tokenizer e .onnx).
      - ./quantized_finbert:/model
    working_dir: /app
    depends_on:
      - kafka
    container_name: sentiment_bluesky
    restart: always
    environment:
      # *** QUESTA È LA MODIFICA CHIAVE ***
      # Dice a PyFlink di includere i JAR dalle directory standard di Flink
      # (/opt/flink/lib e /opt/flink/opt) nella classpath.
      # Il connettore Kafka verrà copiato in /opt/flink/lib dal Dockerfile.
      - PYFLINK_CLASSPATH=/opt/flink/lib/*:/opt/flink/opt/*  
      - FINBERT_MODEL_BASE_PATH=/model 
    mem_limit: 4g # Consider increasing this if you raise parallelism
    cpus: 1 

  # sentiment_reddit:
  #   build:
  #     context: .
  #     dockerfile: ./sentiment_reddit/Dockerfile
  #   volumes:
  #     # Mount per il tuo codice Python.
  #     - ./sentiment_reddit:/app
  #     # Mount per la cartella del modello (tokenizer e .onnx).
  #     - ./quantized_finbert:/model
  #   working_dir: /app
  #   depends_on:
  #     - kafka
  #   container_name: sentiment_reddit
  #   restart: always
  #   environment:
  #     # *** QUESTA È LA MODIFICA CHIAVE ***
  #     # Dice a PyFlink di includere i JAR dalle directory standard di Flink
  #     # (/opt/flink/lib e /opt/flink/opt) nella classpath.
  #     # Il connettore Kafka verrà copiato in /opt/flink/lib dal Dockerfile.
  #     - PYFLINK_CLASSPATH=/opt/flink/lib/*:/opt/flink/opt/*  
  #     - FINBERT_MODEL_BASE_PATH=/model 
  #   mem_limit: 4g # Consider increasing this if you raise parallelism
  #   cpus: 1

  sentiment_news:
    build:
      context: .
      dockerfile: ./sentiment_news/Dockerfile
    volumes:
      # Mount per il tuo codice Python.
      - ./sentiment_news:/app
      # Mount per la cartella del modello (tokenizer e .onnx).
      - ./quantized_finbert:/model
    working_dir: /app
    depends_on:
      - kafka
    container_name: sentiment_news
    restart: always
    environment:
      # *** QUESTA È LA MODIFICA CHIAVE ***
      # Dice a PyFlink di includere i JAR dalle directory standard di Flink
      # (/opt/flink/lib e /opt/flink/opt) nella classpath.
      # Il connettore Kafka verrà copiato in /opt/flink/lib dal Dockerfile.
      - PYFLINK_CLASSPATH=/opt/flink/lib/*:/opt/flink/opt/*  
      - FINBERT_MODEL_BASE_PATH=/model 
    mem_limit: 4g # Consider increasing this if you raise parallelism
    cpus: 1 
  
  sentiment_consumer:
    build:
      context: ./consumer_sentiment
    depends_on:
      - kafka
      - minio
    environment:
      S3_ENDPOINT: http://minio:9000
      S3_ACCESS_KEY: admin
      S3_SECRET_KEY: admin123
      S3_BUCKET: sentiment-data
    networks:
      - default  # o il nome della rete Docker se ne hai definita una custom
    restart: always

# ### AGGREGATION LAYER

  # flink:
  #   build:
  #     context: ./preprocessing_stream
  #   container_name: flink
  #   depends_on: 
  #     - kafka
  #   restart: always

  flink_main_job:
    build:
      context: ./preprocessing_stream2 
      dockerfile: ./main_job.Dockerfile
    container_name: flink_main_job
    depends_on:
      - kafka
    restart: always

  flink-global-job:
    build:
      context: ./preprocessing_stream2
      dockerfile: ./global_job.Dockerfile
    container_name: flink-global-job
    depends_on:
      - kafka
    restart: always

  flink-aggregated-job:
    build:
      context: ./preprocessing_stream2
      dockerfile: ./aggregated_job.Dockerfile
    container_name: flink-aggregated-job
    depends_on:
      - kafka
    restart: always

  aggregated_storage:
    build: ./consumer_aggregated
    container_name: aggregated_storage
    depends_on:
      - kafka 
      - postgre
    command: python consumer_aggregated.py
    restart: always
    environment: # <-- Aggiungi questa sezione
      - POSTGRES_HOST=postgre
      - POSTGRES_DB=aggregated-data
      - POSTGRES_USER=admin
      - POSTGRES_PASSWORD=admin123
      - POSTGRES_PORT=5432

### MODEL TRAINING

  # lstm_trainer:
  #   build:
  #     context: ./create_model_lstm  
  #     dockerfile: Dockerfile
  #   depends_on:
  #     - postgre
  #   environment:
  #     - DB_HOST=postgre
  #     - DB_PORT=5432
  #     - DB_NAME=aggregated-data
  #     - DB_USER=admin
  #     - DB_PASSWORD=admin123
  #   volumes:
  #     - ./create_model_lstm/model:/app/model
  #   #command: python train_lstm.py 
  #   deploy:
  #     resources:
  #       limits:
  #         memory: 4G # Aumenta la memoria disponibile, ad esempio a 4GB o più
  #         cpus: '3.0' # Alloca 2 core CPU, puoi aumentare o diminuire a seconda delle tue risorse
  #       reservations:
  #         memory: 4G # Riserva 2GB di memoria
  #         cpus: '3.0' # Riserva 1 core CPU
    # deploy:
    #   resources:
    #     limits:
    #       memory: 8g # Example: Give 4GB of RAM.


  # lstm_trainer:
  #     build:
  #       context: . # Docker build context is now 'my_test_ingestion'
  #       dockerfile: create_model/Dockerfile
  #     depends_on:
  #       - postgre
  #     environment:
  #       POSTGRES_HOST: ${POSTGRES_HOST}  
  #       POSTGRES_PORT: ${POSTGRES_PORT} 
  #       POSTGRES_DB: ${POSTGRES_DB}
  #       POSTGRES_USER: ${POSTGRES_USER}  
  #       POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
  #     volumes:
  #       - ./prediction_layer/models_lstm:/app/output_data 
  #     command: python first_train.py
  #     deploy:
  #       resources:
  #         limits:
  #           memory: 2G
  #           cpus: '3.0'
  #         reservations:
  #           memory: 2G
  #           cpus: '3.0'

### PREDICTION LAYER

#   predictor_layer:
#     build:
#       context: ./prediction_layer 
#     depends_on:
#       - kafka
#     restart: always


  prediction_consumer:
    build:
      context: ./prediction_layer # Il contesto di build è la directory corrente (dove si trova il Dockerfile)
      dockerfile: Dockerfile # Il nome del tuo Dockerfile
    container_name: prediction_layer
    depends_on:
      - kafka # Assicura che Kafka sia avviato prima del consumer
    environment:
      KAFKA_BROKER: kafka:9092 # Questo è CRUCIALE: usa il nome del servizio Kafka e la porta interna
      KAFKA_TOPIC: aggregated_data
    # Se vuoi esporre porte dal tuo consumer (es. per un'API di monitoraggio), aggiungile qui
    # ports:
    #   - "5000:5000"
      # Non è strettamente necessario mappare volumi se gli artefatti sono copiati durante il build
      # ma potresti farlo se vuoi aggiornare modello/scaler senza ricostruire l'immagine
      # - ./model:/app/model
      # - ./your_prediction_script_name.py:/app/your_prediction_script_name.py
    restart: on-failure # Riavvia il container se fallisce


   #consumer prediction
  kc_prediction:
    build: ./consumer_prediction
    depends_on:
      - kafka
      - minio
    environment:
      KAFKA_BOOTSTRAP_SERVERS: ${KAFKA_BOOTSTRAP_SERVERS}
      S3_ENDPOINT_URL: ${S3_ENDPOINT_URL} 
      S3_ACCESS_KEY: ${S3_ACCESS_KEY}
      S3_SECRET_KEY: ${S3_SECRET_KEY}
    volumes:
      - ./consumer_prediction:/app
    working_dir: /app
    command: python consumer_prediction.py
    restart: always

#   prediction_layer:
#     build:
#       context: .
#       dockerfile: ./prediction_layer/Dockerfile
#     container_name: prediction_layer
#     depends_on:
#       - kafka
#     environment:
#       KAFKA_BROKER: kafka:9092
#       KAFKA_TOPIC_AGGREGATED: aggregated_data
#       KAFKA_TOPIC_DASHBOARD: prediction
#       PYFLINK_CLASSPATH:  /opt/flink/lib/*:/opt/flink/opt/*  
#     mem_limit: 4g
#     volumes:
#       - ./create_model_lstm:/app/create_model_lstm
#     restart: unless-stopped
#     cpus: 1 




# ### DASHBOARD 

  dashboard:
    build:
      context: ./dashboard
      dockerfile: Dockerfile
    container_name: dashboard
    ports:
      - "8501:8501"
    depends_on:
      - kafka
    environment:
      - PYTHONUNBUFFERED=1

# ## CLEANER MINIO

  # clear-minio:
  #   build:
  #     context: ./clean_minio
  #   environment:
  #     MINIO_URL: "minio:9000"
  #     MINIO_ACCESS_KEY: "admin"
  #     MINIO_SECRET_KEY: "admin123"
  #   command: ["python", "clean_minio.py"]


# HISTORICAL CONTAINER

# Historical producer

  # kp_h_macrodata:
  #   build: ./kp_historical_macrodata
  #   environment:
  #     KAFKA_BOOTSTRAP_SERVERS: ${KAFKA_BOOTSTRAP_SERVERS}
  #     API_KEY_FRED: ${API_KEY_FRED}
  #   depends_on:
  #     - kafka

  # kp_h_alpaca:
  #   build: ./kp_historical_alpaca
  #   environment:
  #     KAFKA_BOOTSTRAP_SERVERS: ${KAFKA_BOOTSTRAP_SERVERS}
  #     API_KEY_ALPACA: ${API_KEY_ALPACA}
  #     API_SECRET_ALPACA: ${API_SECRET_ALPACA}
  #     POSTGRES_HOST: ${POSTGRES_HOST} 
  #     POSTGRES_PORT: ${POSTGRES_PORT} 
  #     POSTGRES_DB: ${POSTGRES_DB}   
  #     POSTGRES_USER: ${POSTGRES_USER} 
  #     POSTGRES_PASSWORD: ${POSTGRES_PASSWORD} 
  #   depends_on:
  #     - kafka
  #     - postgre

  # kp_h_company:
  #   build:
  #     context: ./kp_historical_company
  #   container_name: kp_h_company
  #   environment:
  #     KAFKA_BOOTSTRAP_SERVERS: ${KAFKA_BOOTSTRAP_SERVERS}
  #   depends_on:
  #     - kafka

# # # historical consumer
#   kc_h_company:
#     build:
#       context: ./kc_historical_company
#     container_name: kc_h_company
#     depends_on:
#       - kafka
#       - minio
#     environment:
#       KAFKA_BOOTSTRAP_SERVERS: ${KAFKA_BOOTSTRAP_SERVERS}
#       S3_ENDPOINT_URL: ${S3_ENDPOINT_URL} 
#       S3_ACCESS_KEY: ${S3_ACCESS_KEY} 
#       S3_SECRET_KEY: ${S3_SECRET_KEY}
      
#   kc_h_alpaca:
#     build:
#       context: ./kc_historical_alpaca
#     container_name: kc_h_alpaca
#     depends_on:
#       - kafka
#       - minio
#     environment:
#       KAFKA_BOOTSTRAP_SERVERS: ${KAFKA_BOOTSTRAP_SERVERS}
#       S3_ENDPOINT_URL: ${S3_ENDPOINT_URL} 
#       S3_ACCESS_KEY: ${S3_ACCESS_KEY} 
#       S3_SECRET_KEY: ${S3_SECRET_KEY}

#   kc_h_macrodata:
#     build:
#       context: ./kc_historical_macrodata
#     container_name: kc_h_macrodata
#     depends_on:
#       - kafka
#       - minio
#     environment:
#       KAFKA_BOOTSTRAP_SERVERS: ${KAFKA_BOOTSTRAP_SERVERS}
#       S3_ENDPOINT: ${S3_ENDPOINT} 
#       S3_ACCESS_KEY: ${S3_ACCESS_KEY} 
#       S3_SECRET_KEY: ${S3_SECRET_KEY}
#       SPARK_SUBMIT_OPTIONS: --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0

  # flink_historical:
  #     build:
  #       context: ./historical_aggregated
  #     container_name: flink_historical
  #     depends_on:
  #       - kafka
  #       - postgre
  #     restart: always
  #     environment:
  #       KAFKA_BOOTSTRAP_SERVERS: ${KAFKA_BOOTSTRAP_SERVERS}
  #       POSTGRES_HOST: ${DB_HOST} 
  #       POSTGRES_PORT: ${DB_PORT} 
  #       POSTGRES_DB: ${DB_NAME}   
  #       POSTGRES_USER: ${DB_USER} 
  #       POSTGRES_PASSWORD: ${DB_PASSWORD} 


networks:
  flink_net:
    driver: bridge

  minio_network:
    driver: bridge


volumes:
  pgdata:
    driver: local 