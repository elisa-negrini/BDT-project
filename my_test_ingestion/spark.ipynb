{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d19f208",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col, to_json, struct\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, TimestampType\n",
    "from pyspark.sql.streaming import GroupState, GroupStateTimeout\n",
    "from pyspark.sql.functions import expr\n",
    "from pyspark.sql.types import Row\n",
    "\n",
    "# Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"StatefulAnomalyDetection\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Schema\n",
    "schema = StructType([\n",
    "    StructField(\"ticker\", StringType(), True),\n",
    "    StructField(\"price\", DoubleType(), True),\n",
    "    StructField(\"timestamp\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Kafka input\n",
    "kafka_streaming_df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "    .option(\"subscribe\", \"test_topic\") \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .load()\n",
    "\n",
    "# Parse messages\n",
    "parsed_df = kafka_streaming_df.selectExpr(\"CAST(value AS STRING)\") \\\n",
    "    .select(from_json(col(\"value\"), schema).alias(\"data\")) \\\n",
    "    .select(\"data.*\") \\\n",
    "    .withColumn(\"timestamp\", col(\"timestamp\").cast(TimestampType()))\n",
    "\n",
    "# Define update function\n",
    "def detect_anomalies(ticker, rows, state: GroupState):\n",
    "    prev_price = state.get(\"prev_price\") if state.exists else None\n",
    "    output = []\n",
    "\n",
    "    for row in rows:\n",
    "        current_price = row.price\n",
    "        timestamp = row.timestamp\n",
    "\n",
    "        if prev_price is not None:\n",
    "            change_pct = ((current_price - prev_price) / prev_price) * 100\n",
    "            if abs(change_pct) > 10:\n",
    "                output.append(Row(ticker=ticker,\n",
    "                                  price=current_price,\n",
    "                                  prev_price=prev_price,\n",
    "                                  price_change_pct=change_pct,\n",
    "                                  timestamp=timestamp))\n",
    "\n",
    "        # update state\n",
    "        state.update({\"prev_price\": current_price})\n",
    "\n",
    "    return output\n",
    "\n",
    "# Apply stateful logic\n",
    "from pyspark.sql.functions import expr\n",
    "from pyspark.sql.types import StructType\n",
    "\n",
    "output_schema = StructType([\n",
    "    StructField(\"ticker\", StringType()),\n",
    "    StructField(\"price\", DoubleType()),\n",
    "    StructField(\"prev_price\", DoubleType()),\n",
    "    StructField(\"price_change_pct\", DoubleType()),\n",
    "    StructField(\"timestamp\", TimestampType())\n",
    "])\n",
    "\n",
    "anomalies = parsed_df.groupBy(\"ticker\").applyInPandasWithState(\n",
    "    detect_anomalies,\n",
    "    output_schema,\n",
    "    stateStructType=StructType([StructField(\"prev_price\", DoubleType())]),\n",
    "    outputMode=\"append\",\n",
    "    timeoutConf=\"10 minutes\"\n",
    ")\n",
    "\n",
    "# Output to console\n",
    "query = anomalies.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", False) \\\n",
    "    .start()\n",
    "\n",
    "print(\"âœ… Streaming con stato attivo: rilevamento anomalie in corso...\")\n",
    "query.awaitTermination()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
