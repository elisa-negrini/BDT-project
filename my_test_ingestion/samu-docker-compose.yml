services:
  zookeeper:
    image: confluentinc/cp-zookeeper:latest
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"

  kafka:
    image: confluentinc/cp-kafka:latest
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
    #volumes:
    # - ./create-topics.sh:/tmp/create-topics.sh
    #command: ["bash", "/tmp/create-topics.sh"]

  spark-master:
    image: bitnami/spark:latest
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    ports:
      - "7077:7077"
      - "8080:8080"  # Spark master web UI

  spark-worker:
    image: bitnami/spark:latest
    depends_on:
      - spark-master
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=1G
      - SPARK_WORKER_CORES=1
    ports:
      - "8081:8081"  # Spark worker web UI
    
  # spark-writer:
  #   build: ./spark-writer
  #   depends_on:
  #      - spark-master
  #      - spark-worker
  #      - kafka
  #      - minio
  #   environment:
  #     AWS_ACCESS_KEY_ID: admin
  #     AWS_SECRET_ACCESS_KEY: admin123
  #     SPARK_PROPERTIES: |
  #       {
  #         "spark.hadoop.fs.s3a.endpoint":"http://minio:9000",
  #         "spark.hadoop.fs.s3a.path.style.access":"true",
  #         "spark.hadoop.fs.s3a.impl":"org.apache.hadoop.fs.s3a.S3AFileSystem"
  #       }
  #   volumes:
  #     - ./spark-writer:/app
  #   command: >
  #     /opt/spark/bin/spark-submit --master spark://spark-master:7077 /app/spark_writer.py


  # Kafka producer data_reddit
  kafka-producer_reddit:
    build: ./producer_reddit
    environment:
      KAFKA_SERVER: kafka:9092
    depends_on:
      - kafka
  
  # Kafka producer data_finnhubnews
  kafka-producer_finnhubnews:
    build: ./producer_finnhubnews
    environment:
      KAFKA_SERVER: kafka:9092
    depends_on:
      - kafka

  # Kafka producer data_alpaca
  kafka-producer_alpaca:
    build: ./producer_alpaca
    environment:
      KAFKA_SERVER: kafka:9092
    depends_on:
      - kafka

  # Kafka producer bluesky
  kafka-producer_bluesky:
    build: ./producer_bluesky
    environment:
      KAFKA_SERVER: kafka:9092
    depends_on:
      - kafka
    
  # Kafka producer macrodata
  kafka-producer_macrodata:
    build: ./producer_macrodata
    environment:
      KAFKA_SERVER: kafka:9092
    depends_on:
      - kafka

  # Kafka consumer finnhubnews
  kafka-consumer_finnhubnews:
    build: ./consumer_finnhubnews
    environment:
      KAFKA_SERVER: kafka:9092
      S3_ENDPOINT: http://minio:9000
      S3_ACCESS_KEY: admin
      S3_SECRET_KEY: admin123
      S3_BUCKET: finnhub-data
    depends_on:
      - kafka
      - minio
    volumes:
      - ./consumer_finnhubnews:/app
    working_dir: /app
    command: python consumer_finnhubnews.py

  # Kafka consumer sentiment
  sentiment_bluesky:
    build: ./sentiment_bluesky
    depends_on:
      - kafka
      - spark-master
      - spark-worker
    volumes:
      - ./sentiment_bluesky:/app
    working_dir: /app
    command: >
      spark-submit
      --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0
      sentiment_bluesky.py
    environment:
      - PYSPARK_PYTHON=python3

  sentiment_reddit:
    build: ./sentiment_reddit
    depends_on:
      - kafka
      - spark-master
      - spark-worker
    volumes:
      - ./sentiment_reddit:/app
    working_dir: /app
    command: >
      spark-submit
      --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0
      sentiment_reddit.py
    environment:
       - PYSPARK_PYTHON=python3


  sentiment_news:
    build: ./sentiment_news
    depends_on:
      - kafka
      - spark-master
      - spark-worker
    volumes:
      - ./sentiment_news:/app
    working_dir: /app
    command: >
      spark-submit
      --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0
      sentiment_news.py
    environment:
       - PYSPARK_PYTHON=python3

    
  # Kafka consumer alpaca
  kafka-consumer_alpaca:
    build: ./consumer_alpaca
    depends_on:
      - kafka
      - minio
    environment:
      KAFKA_SERVER: kafka:9092
      S3_ENDPOINT: http://minio:9000
      S3_ACCESS_KEY: admin
      S3_SECRET_KEY: admin123
      S3_BUCKET: stock-data  # Usa il nome del bucket che vuoi utilizzare
    volumes:
      - ./consumer_alpaca:/app
    working_dir: /app
    command: python consumer_alpaca.py

  # Kafka consumer reddit
  kafka-consumer_reddit:
    build: ./consumer_reddit 
    environment:
      KAFKA_SERVER: kafka:9092
      S3_ENDPOINT: http://minio:9000
      S3_ACCESS_KEY: admin
      S3_SECRET_KEY: admin123
      S3_BUCKET: reddit-data
    depends_on:
      - kafka
      - minio
    volumes:
      - ./consumer_reddit:/app
    working_dir: /app
    command: python consumer_reddit.py

  #kafka consumer bluesky
  kafka-consumer_bluesky:
    build: ./consumer_bluesky
    depends_on:
      - kafka
      - minio
    environment:
      KAFKA_SERVER: kafka:9092
      S3_ENDPOINT: http://minio:9000
      S3_ACCESS_KEY: admin
      S3_SECRET_KEY: admin123
      S3_BUCKET: bluesky-data
    volumes:
      - ./consumer_bluesky:/app
    working_dir: /app
    command: python consumer_bluesky.py

  #kafka consumer bluesky
  kafka-consumer_macrodata:
    build: ./consumer_macrodata
    depends_on:
      - kafka
      - minio
    environment:
      KAFKA_SERVER: kafka:9092
      S3_ENDPOINT: http://minio:9000
      S3_ACCESS_KEY: admin
      S3_SECRET_KEY: admin123
      S3_BUCKET: macro-data
    volumes:
      - ./consumer_macrodata:/app
    working_dir: /app
    command: python consumer_macrodata.py


  minio:
    image: minio/minio
    container_name: minio
    ports:
     - "9000:9000"   # S3 API
     - "9001:9001"   # MinIO Console (GUI)
    environment:
      MINIO_ROOT_USER: admin
      MINIO_ROOT_PASSWORD: admin123
    volumes:
     - ./data/minio:/data
    command: server /data --console-address ":9001"



  # spark-app:
  #   build: ./spark-app
  #   depends_on:
  #     - kafka
  #     - spark-master
  #     - spark-worker
  #   environment:
  #     - PYSPARK_PYTHON=python3
  #   volumes:
  #     - ./spark-app:/app
  #   command: >
  #     bash -c "spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.4 /app/app.py"



