services:
  zookeeper:
    image: confluentinc/cp-zookeeper:latest
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"

  kafka:
    image: confluentinc/cp-kafka:latest
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
    environment:
          KAFKA_BROKER_ID: 1
          KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
          KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
          KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
    restart: always

  minio:
    image: minio/minio
    ports:
     - "9000:9000"
     - "9001:9001"
    environment:
      MINIO_ROOT_USER: ${S3_ACCESS_KEY} 
      MINIO_ROOT_PASSWORD: ${S3_SECRET_KEY}
    volumes:
     - ./data/minio:/data
    command: server /data --console-address ":9001"

  postgres:
    build:
      context: ./postgresql
      dockerfile: Dockerfile
    environment:
      POSTGRES_DB: ${DB_NAME}
      POSTGRES_USER: ${DB_USER}
      POSTGRES_PASSWORD: ${DB_PASSWORD}
    ports:
      - "5432:5432"
    volumes:
      - pgdata:/var/lib/postgresql/data
    restart: always

### PRODUCER STREAM
  producer_reddit: # tutto sistemato
    build: ./stream_data/producer_reddit
    environment:
      REDDIT_CLIENT_ID: ${REDDIT_CLIENT_ID}
      REDDIT_CLIENT_SECRET: ${REDDIT_CLIENT_SECRET}
      REDDIT_USER_AGENT: ${REDDIT_USER_AGENT}
    depends_on:
      - kafka
    restart: always

  producer_finnhubnews: # tutto sistemato
    build: ./stream_data/producer_finnhubnews
    environment:
      FINNHUB_API_KEY: ${FINNHUB_API_KEY}
      POSTGRES_HOST: ${POSTGRES_HOST} 
      POSTGRES_PORT: ${POSTGRES_PORT} 
      POSTGRES_DB: ${POSTGRES_DB}   
      POSTGRES_USER: ${POSTGRES_USER} 
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD} 
    depends_on:
      - kafka
      - postgres
    restart: always

  producer_alpaca: # tutto sistemato
    build: ./stream_data/producer_alpaca
    environment:
      API_KEY_ALPACA: ${API_KEY_ALPACA}
      API_SECRET_ALPACA: ${API_SECRET_ALPACA}
      POSTGRES_HOST: ${POSTGRES_HOST} 
      POSTGRES_PORT: ${POSTGRES_PORT} 
      POSTGRES_DB: ${POSTGRES_DB}   
      POSTGRES_USER: ${POSTGRES_USER} 
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD} 
    depends_on:
      - kafka
      - postgres
    restart: always

  producer_bluesky: # tutto sistemato
    build: ./stream_data/producer_bluesky
    environment:
      BLUESKY_IDENTIFIER: ${BLUESKY_IDENTIFIER}
      BLUESKY_PASSWORD: ${BLUESKY_PASSWORD}
      POSTGRES_HOST: ${POSTGRES_HOST} 
      POSTGRES_PORT: ${POSTGRES_PORT} 
      POSTGRES_DB: ${POSTGRES_DB}   
      POSTGRES_USER: ${POSTGRES_USER} 
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD} 
    depends_on:
      - kafka
      - postgres
    restart: always

  producer_macrodata: # tutto sistemato
    build: ./stream_data/producer_macrodata
    environment:
      API_KEY_FRED: ${API_KEY_FRED}
    depends_on:
      - kafka
    restart: always

  consumer_finnhubnews: # tutto sistemato
    build: ./stream_data/consumer_finnhubnews
    environment:
      S3_ENDPOINT_URL: ${S3_ENDPOINT_URL} 
      S3_ACCESS_KEY: ${S3_ACCESS_KEY}
      S3_SECRET_KEY: ${S3_SECRET_KEY}
    volumes:
      - ./stream_data/consumer_finnhubnews:/app
    working_dir: /app
    #command: python consumer_finnhubnews.py
    depends_on:
      - kafka
      - minio
      - producer_finnhubnews
    restart: always

  consumer_alpaca: # tutto sistemato
    build: ./stream_data/consumer_alpaca
    environment:
      S3_ENDPOINT_URL: ${S3_ENDPOINT_URL} 
      S3_ACCESS_KEY: ${S3_ACCESS_KEY}
      S3_SECRET_KEY: ${S3_SECRET_KEY}
    volumes:
      - ./stream_data/consumer_alpaca:/app
    working_dir: /app
    #command: python consumer_alpaca.py
    depends_on:
      - kafka
      - minio
      - producer_alpaca
    restart: always

  consumer_reddit: # tutto sistemato
    build: ./stream_data/consumer_reddit 
    environment:
      S3_ENDPOINT_URL: ${S3_ENDPOINT_URL} 
      S3_ACCESS_KEY: ${S3_ACCESS_KEY}
      S3_SECRET_KEY: ${S3_SECRET_KEY}
    volumes:
      - ./stream_data/consumer_reddit:/app
    working_dir: /app
    #command: python consumer_reddit.py
    depends_on:
      - kafka
      - minio
      - producer_reddit
    restart: always

  consumer_bluesky: # tutto sistemato
    build: ./stream_data/consumer_bluesky
    environment:
      S3_ENDPOINT_URL: ${S3_ENDPOINT_URL} 
      S3_ACCESS_KEY: ${S3_ACCESS_KEY}
      S3_SECRET_KEY: ${S3_SECRET_KEY}
    volumes:
      - ./stream_data/consumer_bluesky:/app
    working_dir: /app
    #command: python consumer_bluesky.py
    depends_on:
      - kafka
      - minio
      - producer_bluesky
    restart: always

  consumer_macrodata: # da togliere ultima parte commentata(?)
    build: ./stream_data/consumer_macrodata
    environment:
      S3_ENDPOINT_URL: ${S3_ENDPOINT_URL} 
      S3_ACCESS_KEY: ${S3_ACCESS_KEY}
      S3_SECRET_KEY: ${S3_SECRET_KEY}
    volumes:
      - ./stream_data/consumer_macrodata:/app
    working_dir: /app
    #command: python consumer_macrodata.py
    depends_on:
      - kafka
      - minio
      - producer_macrodata
    restart: always

## SENTIMENT ANALYSIS

  # sentiment_bluesky:
  #   build:
  #     context: .
  #     dockerfile: ./sentiment_bluesky/Dockerfile
  #   volumes:
  #     # Mount per il tuo codice Python.
  #     - ./sentiment_bluesky:/app
  #     # Mount per la cartella del modello (tokenizer e .onnx).
  #     - ./quantized_finbert:/model
  #   working_dir: /app
  #   depends_on:
  #     - kafka
  #     - postgres
  #   container_name: sentiment_bluesky
  #   restart: always
  #   environment:
  #     # *** QUESTA È LA MODIFICA CHIAVE ***
  #     # Dice a PyFlink di includere i JAR dalle directory standard di Flink
  #     # (/opt/flink/lib e /opt/flink/opt) nella classpath.
  #     # Il connettore Kafka verrà copiato in /opt/flink/lib dal Dockerfile.
  #     - PYFLINK_CLASSPATH=/opt/flink/lib/:/opt/flink/opt/  
  #     - FINBERT_MODEL_BASE_PATH=/model 
  #     - POSTGRES_HOST=${DB_HOST}  
  #     - POSTGRES_PORT=${DB_PORT} 
  #     - POSTGRES_DB=${DB_NAME}   # Il nome del database nel tuo servizio 'postgre'
  #     - POSTGRES_USER=${DB_USER}  
  #     - POSTGRES_PASSWORD=${DB_PASSWORD} 
  #   mem_limit: 4g # Consider increasing this if you raise parallelism
  #   cpus: 1 


  # # sentiment_reddit:
  # #   build:
  # #     context: .
  # #     dockerfile: ./sentiment_reddit/Dockerfile
  # #   volumes:
  # #     # Mount per il tuo codice Python.
  # #     - ./sentiment_reddit:/app
  # #     # Mount per la cartella del modello (tokenizer e .onnx).
  # #     - ./quantized_finbert:/model
  # #   working_dir: /app
  # #   depends_on:
  # #     - kafka
  # #     - postgres
  # #   container_name: sentiment_reddit
  # #   restart: always
  # #   environment:
  # #     # *** QUESTA È LA MODIFICA CHIAVE ***
  # #     # Dice a PyFlink di includere i JAR dalle directory standard di Flink
  # #     # (/opt/flink/lib e /opt/flink/opt) nella classpath.
  # #     # Il connettore Kafka verrà copiato in /opt/flink/lib dal Dockerfile.
  # #     - PYFLINK_CLASSPATH=/opt/flink/lib/:/opt/flink/opt/  
  # #     - FINBERT_MODEL_BASE_PATH=/model 
  # #   mem_limit: 4g # Consider increasing this if you raise parallelism
  # #   cpus: 1


 # sentiment_news:
  #   build:
  #     context: .
  #     dockerfile: ./sentiment_news/Dockerfile
  #   volumes:
  #     # Mount per il tuo codice Python.
  #     - ./sentiment_news:/app
  #     # Mount per la cartella del modello (tokenizer e .onnx).
  #     - ./quantized_finbert:/model
  #   working_dir: /app
  #   depends_on:
  #     - kafka
  #     - postgres
  #   container_name: sentiment_news
  #   restart: always
  #   environment:
  #     # *** QUESTA È LA MODIFICA CHIAVE ***
  #     # Dice a PyFlink di includere i JAR dalle directory standard di Flink
  #     # (/opt/flink/lib e /opt/flink/opt) nella classpath.
  #     # Il connettore Kafka verrà copiato in /opt/flink/lib dal Dockerfile.
  #     - PYFLINK_CLASSPATH=/opt/flink/lib/:/opt/flink/opt/  
  #     - FINBERT_MODEL_BASE_PATH=/model 
  #     - POSTGRES_HOST=${DB_HOST}  
  #     - POSTGRES_PORT=${DB_PORT} 
  #     - POSTGRES_DB=${DB_NAME}   # Il nome del database nel tuo servizio 'postgre'
  #     - POSTGRES_USER=${DB_USER}  
  #     - POSTGRES_PASSWORD=${DB_PASSWORD} 
  #   mem_limit: 4g # Consider increasing this if you raise parallelism
  #   cpus: 1 
 
  
  # sentiment_consumer:
  #   build:
  #     context: ./consumer_sentiment
  #   depends_on:
  #     - kafka
  #     - minio
  #   environment:
  #     S3_ENDPOINT_URL: ${S3_ENDPOINT_URL} 
  #     S3_ACCESS_KEY: ${S3_ACCESS_KEY}
  #     S3_SECRET_KEY: ${S3_SECRET_KEY}
  #   networks:
  #     - default
  #   restart: always


# ### AGGREGATION LAYER

  # flink:
  #   build:
  #     context: ./preprocessing_stream
  #   container_name: flink
  #   depends_on: 
  #     - kafka
  #   restart: always

  # flink_main_job:
  #   build:
  #     context: ./preprocessing_stream2 
  #     dockerfile: ./main_job.Dockerfile
  #   container_name: flink_main_job
  #   depends_on:
  #     - kafka
  #     - postgres
  #     - minio
  #   environment:
  #     POSTGRES_HOST: ${POSTGRES_HOST}  
  #     POSTGRES_PORT: ${POSTGRES_PORT} 
  #     POSTGRES_DB: ${POSTGRES_DB}
  #     POSTGRES_USER: ${POSTGRES_USER}  
  #     POSTGRES_PASSWORD: ${POSTGRES_PASSWORD} 
  #     S3_ENDPOINT: ${S3_ENDPOINT} 
  #     S3_ACCESS_KEY: ${S3_ACCESS_KEY}
  #     S3_SECRET_KEY: ${S3_SECRET_KEY}
  #   restart: always

  # flink-global-job:
  #   build:
  #     context: ./preprocessing_stream2
  #     dockerfile: ./global_job.Dockerfile
  #   container_name: flink-global-job
  #   depends_on:
  #     - kafka
  #   restart: always

  # flink-aggregated-job:
  #   build:
  #     context: ./preprocessing_stream2
  #     dockerfile: ./aggregated_job.Dockerfile
  #   container_name: flink-aggregated-job
  #   depends_on:
  #     - kafka
  #   restart: always

    # aggregated_storage:
    #   build: ./consumer_aggregated
    #   container_name: aggregated_storage
    #   depends_on:
    #     - kafka 
    #     - postgres
    #   command: python consumer_aggregated.py
    #   restart: always
    #   environment: # <-- Aggiungi questa sezione
    #     POSTGRES_HOST: ${POSTGRES_HOST}  
    #     POSTGRES_PORT: ${POSTGRES_PORT} 
    #     POSTGRES_DB: ${POSTGRES_DB}
    #     POSTGRES_USER: ${POSTGRES_USER}  
    #     POSTGRES_PASSWORD: ${POSTGRES_PASSWORD} 

### MODEL TRAINING


  # lstm_trainer:
  #   build:
  #     context: . # Docker build context is now 'my_test_ingestion'
  #     dockerfile: create_model_lstm/Dockerfile
  #   depends_on:
  #     - postgres
  #   environment:
  #     POSTGRES_HOST: ${POSTGRES_HOST}  
  #     POSTGRES_PORT: ${POSTGRES_PORT} 
  #     POSTGRES_DB: ${POSTGRES_DB}
  #     POSTGRES_USER: ${POSTGRES_USER}  
  #     POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
  #   volumes:
  #     - ./prediction_layer/models_lstm:/app/output_data 
  #   command: python train_lstm.py
  #   deploy:
  #     resources:
  #       limits:
  #         memory: 4G
  #         cpus: '3.0'
  #       reservations:
  #         memory: 4G
  #         cpus: '3.0' 


### PREDICTION LAYER

#   predictor_layer:
#     build:
#       context: ./prediction_layer 
#     depends_on:
#       - kafka
#     restart: always


  # prediction_consumer:
  #   build:
  #     context: ./prediction_layer # Il contesto di build è la directory corrente (dove si trova il Dockerfile)
  #     dockerfile: Dockerfile # Il nome del tuo Dockerfile
  #   container_name: prediction_layer
  #   depends_on:
  #     - kafka # Assicura che Kafka sia avviato prima del consumer
  #   environment:
  #     KAFKA_BROKER: kafka:9092 # Questo è CRUCIALE: usa il nome del servizio Kafka e la porta interna
  #     KAFKA_TOPIC: aggregated_data
  #   # Se vuoi esporre porte dal tuo consumer (es. per un'API di monitoraggio), aggiungile qui
  #   # ports:
  #   #   - "5000:5000"
  #     # Non è strettamente necessario mappare volumi se gli artefatti sono copiati durante il build
  #     # ma potresti farlo se vuoi aggiornare modello/scaler senza ricostruire l'immagine
  #     # - ./model:/app/model
  #     # - ./your_prediction_script_name.py:/app/your_prediction_script_name.py
  #   restart: on-failure # Riavvia il container se fallisce


   #consumer prediction
  # kc_prediction:
  #   build: ./consumer_prediction
  #   depends_on:
  #     - kafka
  #     - minio
  #   environment:
  #     KAFKA_BOOTSTRAP_SERVERS: ${KAFKA_BOOTSTRAP_SERVERS}
  #     S3_ENDPOINT_URL: ${S3_ENDPOINT_URL} 
  #     S3_ACCESS_KEY: ${S3_ACCESS_KEY}
  #     S3_SECRET_KEY: ${S3_SECRET_KEY}
  #   volumes:
  #     - ./consumer_prediction:/app
  #   working_dir: /app
  #   command: python consumer_prediction.py
  #   restart: always

#   prediction_layer:
#     build:
#       context: .
#       dockerfile: ./prediction_layer/Dockerfile
#     container_name: prediction_layer
#     depends_on:
#       - kafka
#     environment:
#       KAFKA_BROKER: kafka:9092
#       KAFKA_TOPIC_AGGREGATED: aggregated_data
#       KAFKA_TOPIC_DASHBOARD: prediction
#       PYFLINK_CLASSPATH:  /opt/flink/lib/*:/opt/flink/opt/*  
#     mem_limit: 4g
#     volumes:
#       - ./create_model_lstm:/app/create_model_lstm
#     restart: unless-stopped
#     cpus: 1 




# ### DASHBOARD 

  # dashboard:
  #   build:
  #     context: ./dashboard
  #     dockerfile: Dockerfile
  #   container_name: dashboard
  #   ports:
  #     - "8501:8501"
  #   depends_on:
  #     - kafka
  #   environment:
  #     - PYTHONUNBUFFERED=1

# ## CLEANER MINIO

  # clear-minio:
  #   build:
  #     context: ./clean_minio
  #   environment:
  #     MINIO_URL: "minio:9000"
  #     MINIO_ACCESS_KEY: "admin"
  #     MINIO_SECRET_KEY: "admin123"
  #   command: ["python", "clean_minio.py"]


# HISTORICAL CONTAINER

# Historical producer

  producer_h_macrodata: # tutto sistemato
    build: ./historical_data/producer_h_macrodata
    environment:
      API_KEY_FRED: ${API_KEY_FRED}
    depends_on:
      - kafka


  producer_h_alpaca: # tutto sistemato
    build: ./historical_data/producer_h_alpaca
    environment:
      API_KEY_ALPACA: ${API_KEY_ALPACA}
      API_SECRET_ALPACA: ${API_SECRET_ALPACA}
      POSTGRES_HOST: ${POSTGRES_HOST} 
      POSTGRES_PORT: ${POSTGRES_PORT} 
      POSTGRES_DB: ${POSTGRES_DB}   
      POSTGRES_USER: ${POSTGRES_USER} 
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD} 
    depends_on:
      - kafka
      - postgres


  producer_h_company: # tutto sistemato
    build:
      context: ./historical_data/producer_h_company
    depends_on:
      - kafka


# historical consumer
  consumer_h_company: # tutto sistemato
    build:
      context: ./historical_data/consumer_h_company
    environment:
      S3_ENDPOINT_URL: ${S3_ENDPOINT_URL} 
      S3_ACCESS_KEY: ${S3_ACCESS_KEY} 
      S3_SECRET_KEY: ${S3_SECRET_KEY}
    depends_on:
      - kafka
      - minio
      - producer_h_company
         
  consumer_h_alpaca: # tutto sistemato
    build:
      context: ./historical_data/consumer_h_alpaca
    environment:
      S3_ENDPOINT_URL: ${S3_ENDPOINT_URL} 
      S3_ACCESS_KEY: ${S3_ACCESS_KEY} 
      S3_SECRET_KEY: ${S3_SECRET_KEY}
    depends_on:
      - kafka
      - minio
      - producer_h_alpaca

  consumer_h_macrodata:
    build:
      context: ./historical_data/consumer_h_macrodata
    environment:
      S3_ENDPOINT: ${S3_ENDPOINT} 
      S3_ACCESS_KEY: ${S3_ACCESS_KEY} 
      S3_SECRET_KEY: ${S3_SECRET_KEY}
      SPARK_SUBMIT_OPTIONS: --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0
    depends_on:
      - kafka
      - minio
      - producer_h_macrodata

  # flink_historical:
  #     build:
  #       context: ./historical_data/historical_aggregated
  #     restart: always
  #     environment:
  #       POSTGRES_HOST: ${POSTGRES_HOST} 
  #       POSTGRES_PORT: ${POSTGRES_PORT} 
  #       POSTGRES_DB: ${POSTGRES_DB}   
  #       POSTGRES_USER: ${POSTGRES_USER} 
  #       POSTGRES_PASSWORD: ${POSTGRES_PASSWORD} 
  #     depends_on:
  #       - kafka
  #       - postgres

networks:
  flink_net:
    driver: bridge

  minio_network:
    driver: bridge

volumes:
  pgdata:
    driver: local 