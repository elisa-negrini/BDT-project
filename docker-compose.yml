services:
  zookeeper:
    image: confluentinc/cp-zookeeper:latest
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"

  kafka:
    image: confluentinc/cp-kafka:latest
    ports:
      - "9092:9092"
    environment:
          KAFKA_BROKER_ID: 1
          KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
          KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
          KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
    depends_on:
      - zookeeper
    restart: always

  minio:
    image: minio/minio
    ports:
     - "9000:9000"
     - "9001:9001"
    environment:
      MINIO_ROOT_USER: ${S3_ACCESS_KEY} 
      MINIO_ROOT_PASSWORD: ${S3_SECRET_KEY}
    volumes:
     - ./data/minio:/data
    command: server /data --console-address ":9001"

  postgres:
    build:
      context: ./postgresql
      dockerfile: Dockerfile
    environment:
      POSTGRES_DB: ${DB_NAME}
      POSTGRES_USER: ${DB_USER}
      POSTGRES_PASSWORD: ${DB_PASSWORD}
    ports:
      - "5432:5432"
    volumes:
      - pgdata:/var/lib/postgresql/data
    restart: always

### PRODUCER STREAM
  producer_reddit: # tutto sistemato
    build: ./stream_data/producer_reddit
    environment:
      REDDIT_CLIENT_ID: ${REDDIT_CLIENT_ID}
      REDDIT_CLIENT_SECRET: ${REDDIT_CLIENT_SECRET}
      REDDIT_USER_AGENT: ${REDDIT_USER_AGENT}
    depends_on:
      - kafka
    restart: always

  producer_finnhubnews: # tutto sistemato
    build: ./stream_data/producer_finnhubnews
    environment:
      FINNHUB_API_KEY: ${FINNHUB_API_KEY}
      POSTGRES_HOST: ${POSTGRES_HOST} 
      POSTGRES_PORT: ${POSTGRES_PORT} 
      POSTGRES_DB: ${POSTGRES_DB}   
      POSTGRES_USER: ${POSTGRES_USER} 
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD} 
    depends_on:
      - kafka
      - postgres
    restart: always

  producer_alpaca: # tutto sistemato
    build: ./stream_data/producer_alpaca
    environment:
      API_KEY_ALPACA: ${API_KEY_ALPACA}
      API_SECRET_ALPACA: ${API_SECRET_ALPACA}
      POSTGRES_HOST: ${POSTGRES_HOST} 
      POSTGRES_PORT: ${POSTGRES_PORT} 
      POSTGRES_DB: ${POSTGRES_DB}   
      POSTGRES_USER: ${POSTGRES_USER} 
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD} 
    depends_on:
      - kafka
      - postgres
    restart: always

  producer_bluesky: # tutto sistemato
    build: ./stream_data/producer_bluesky
    environment:
      BLUESKY_IDENTIFIER: ${BLUESKY_IDENTIFIER}
      BLUESKY_PASSWORD: ${BLUESKY_PASSWORD}
      POSTGRES_HOST: ${POSTGRES_HOST} 
      POSTGRES_PORT: ${POSTGRES_PORT} 
      POSTGRES_DB: ${POSTGRES_DB}   
      POSTGRES_USER: ${POSTGRES_USER} 
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD} 
    depends_on:
      - kafka
      - postgres
    restart: always

  producer_macrodata: # tutto sistemato
    build: ./stream_data/producer_macrodata
    environment:
      API_KEY_FRED: ${API_KEY_FRED}
    depends_on:
      - kafka
    restart: always

  consumer_finnhubnews: # tutto sistemato
    build: ./stream_data/consumer_finnhubnews
    environment:
      S3_ENDPOINT_URL: ${S3_ENDPOINT_URL} 
      S3_ACCESS_KEY: ${S3_ACCESS_KEY}
      S3_SECRET_KEY: ${S3_SECRET_KEY}
    volumes:
      - ./stream_data/consumer_finnhubnews:/app
    working_dir: /app
    depends_on:
      - kafka
      - minio
      - producer_finnhubnews
    restart: always

  consumer_alpaca: # tutto sistemato
    build: ./stream_data/consumer_alpaca
    environment:
      S3_ENDPOINT_URL: ${S3_ENDPOINT_URL} 
      S3_ACCESS_KEY: ${S3_ACCESS_KEY}
      S3_SECRET_KEY: ${S3_SECRET_KEY}
    volumes:
      - ./stream_data/consumer_alpaca:/app
    working_dir: /app
    depends_on:
      - kafka
      - minio
      - producer_alpaca
    restart: always

  consumer_reddit: # tutto sistemato
    build: ./stream_data/consumer_reddit 
    environment:
      S3_ENDPOINT_URL: ${S3_ENDPOINT_URL} 
      S3_ACCESS_KEY: ${S3_ACCESS_KEY}
      S3_SECRET_KEY: ${S3_SECRET_KEY}
    volumes:
      - ./stream_data/consumer_reddit:/app
    working_dir: /app
    depends_on:
      - kafka
      - minio
      - producer_reddit
    restart: always

  consumer_bluesky: # tutto sistemato
    build: ./stream_data/consumer_bluesky
    environment:
      S3_ENDPOINT_URL: ${S3_ENDPOINT_URL} 
      S3_ACCESS_KEY: ${S3_ACCESS_KEY}
      S3_SECRET_KEY: ${S3_SECRET_KEY}
    volumes:
      - ./stream_data/consumer_bluesky:/app
    working_dir: /app
    depends_on:
      - kafka
      - minio
      - producer_bluesky
    restart: always

  consumer_macrodata: # da togliere ultima parte commentata(?)
    build: ./stream_data/consumer_macrodata
    environment:
      S3_ENDPOINT_URL: ${S3_ENDPOINT_URL} 
      S3_ACCESS_KEY: ${S3_ACCESS_KEY}
      S3_SECRET_KEY: ${S3_SECRET_KEY}
    volumes:
      - ./stream_data/consumer_macrodata:/app
    working_dir: /app
    depends_on:
      - kafka
      - minio
      - producer_macrodata
    restart: always

## SENTIMENT ANALYSIS

  sentiment_bluesky:
    build:
      context: ./sentiment_analysis
      dockerfile: ./sentiment_analysis/sentiment_bluesky/Dockerfile
    environment:
      - PYFLINK_CLASSPATH=/opt/flink/lib/:/opt/flink/opt/  
      - FINBERT_MODEL_BASE_PATH=/model 
      - POSTGRES_HOST=${DB_HOST}  
      - POSTGRES_PORT=${DB_PORT} 
      - POSTGRES_DB=${DB_NAME}  
      - POSTGRES_USER=${DB_USER}  
      - POSTGRES_PASSWORD=${DB_PASSWORD} 
    volumes:
      - ./sentiment_analysis/sentiment_bluesky:/app
      - ./sentiment_analysis/quantized_finbert:/model
    working_dir: /app
    depends_on:
      - kafka
      - postgres
    restart: always
    mem_limit: 4g
    cpus: 1 

  sentiment_reddit:
    build:
      context: ./sentiment_analysis
      dockerfile: ./sentiment_analysis/sentiment_reddit/Dockerfile
    environment:
      - PYFLINK_CLASSPATH=/opt/flink/lib/:/opt/flink/opt/  
      - FINBERT_MODEL_BASE_PATH=/model 
    volumes:
      - ./sentiment_analysis/sentiment_reddit:/app
      - ./sentiment_analysis/quantized_finbert:/model
    working_dir: /app
    depends_on:
      - kafka
      - postgres
    restart: always
    mem_limit: 4g 
    cpus: 1

  sentiment_news:
      build:
        context: ./sentiment_analysis
        dockerfile: ./sentiment_analysis/sentiment_news/Dockerfile
      environment:
        - PYFLINK_CLASSPATH=/opt/flink/lib/:/opt/flink/opt/  
        - FINBERT_MODEL_BASE_PATH=/model 
        - POSTGRES_HOST=${DB_HOST}  
        - POSTGRES_PORT=${DB_PORT} 
        - POSTGRES_DB=${DB_NAME}
        - POSTGRES_USER=${DB_USER}  
        - POSTGRES_PASSWORD=${DB_PASSWORD} 
      volumes:
        - ./sentiment_analysis/sentiment_news:/app
        - ./sentiment_analysis/quantized_finbert:/model
      working_dir: /app
      depends_on:
        - kafka
        - postgres
      restart: always
      mem_limit: 4g
      cpus: 1 
  
  sentiment_consumer:
    build:
      context: ./sentiment_analysis/consumer_sentiment
    environment:
      S3_ENDPOINT_URL: ${S3_ENDPOINT_URL} 
      S3_ACCESS_KEY: ${S3_ACCESS_KEY}
      S3_SECRET_KEY: ${S3_SECRET_KEY}
    depends_on:
      - kafka
      - minio
    networks:
      - default
    restart: always

# ### AGGREGATION LAYER

  flink_main_job:
    build:
      context: .
      dockerfile: preprocessing_stream/main_job.Dockerfile
    environment:
      POSTGRES_HOST: ${POSTGRES_HOST}  
      POSTGRES_PORT: ${POSTGRES_PORT} 
      POSTGRES_DB: ${POSTGRES_DB}
      POSTGRES_USER: ${POSTGRES_USER}  
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD} 
      S3_ENDPOINT: ${S3_ENDPOINT} 
      S3_ACCESS_KEY: ${S3_ACCESS_KEY}
      S3_SECRET_KEY: ${S3_SECRET_KEY}
    depends_on:
      - kafka
      - postgres
      - minio
    restart: always

  flink-global-job:
    build:
      context: .
      dockerfile: preprocessing_stream/global_job.Dockerfile
    depends_on:
      - kafka
    restart: always

  flink-aggregated-job:
    build:
      context: .
      dockerfile: preprocessing_stream/aggregated_job.Dockerfile
    depends_on:
      - kafka
    restart: always

  storage_aggregated:
    build:
      context: .
      dockerfile: preprocessing_stream/storage_aggregated/Dockerfile
    environment:
      POSTGRES_HOST: ${POSTGRES_HOST}  
      POSTGRES_PORT: ${POSTGRES_PORT} 
      POSTGRES_DB: ${POSTGRES_DB}
      POSTGRES_USER: ${POSTGRES_USER}  
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD} 
    depends_on:
      - kafka 
      - postgres
    restart: always

### MODEL TRAINING
  ML_model:
    build:
      context: .
      dockerfile: ML_model/Dockerfile
    environment:
      POSTGRES_HOST: ${POSTGRES_HOST}  
      POSTGRES_PORT: ${POSTGRES_PORT} 
      POSTGRES_DB: ${POSTGRES_DB}
      POSTGRES_USER: ${POSTGRES_USER}  
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
    volumes:
      - ./prediction_layer/models_lstm:/app/output_data
    depends_on:
      - postgres
    deploy:
      resources:
        limits:
          memory: 4G
          cpus: '3.0'
        reservations:
          memory: 4G
          cpus: '3.0' 


### PREDICTION LAYER

#   predictor_layer:
#     build:
#       context: ./prediction_layer 
#     depends_on:
#       - kafka
#     restart: always


  # prediction_consumer:
  #   build:
  #     context: ./prediction_layer # Il contesto di build è la directory corrente (dove si trova il Dockerfile)
  #     dockerfile: Dockerfile # Il nome del tuo Dockerfile
  #   container_name: prediction_layer
  #   depends_on:
  #     - kafka # Assicura che Kafka sia avviato prima del consumer
  #   environment:
  #     KAFKA_BROKER: kafka:9092 # Questo è CRUCIALE: usa il nome del servizio Kafka e la porta interna
  #     KAFKA_TOPIC: aggregated_data
  #   # Se vuoi esporre porte dal tuo consumer (es. per un'API di monitoraggio), aggiungile qui
  #   # ports:
  #   #   - "5000:5000"
  #     # Non è strettamente necessario mappare volumi se gli artefatti sono copiati durante il build
  #     # ma potresti farlo se vuoi aggiornare modello/scaler senza ricostruire l'immagine
  #     # - ./model:/app/model
  #     # - ./your_prediction_script_name.py:/app/your_prediction_script_name.py
  #   restart: on-failure # Riavvia il container se fallisce


   #consumer prediction
  # kc_prediction:
  #   build: ./consumer_prediction
  #   depends_on:
  #     - kafka
  #     - minio
  #   environment:
  #     KAFKA_BOOTSTRAP_SERVERS: ${KAFKA_BOOTSTRAP_SERVERS}
  #     S3_ENDPOINT_URL: ${S3_ENDPOINT_URL} 
  #     S3_ACCESS_KEY: ${S3_ACCESS_KEY}
  #     S3_SECRET_KEY: ${S3_SECRET_KEY}
  #   volumes:
  #     - ./consumer_prediction:/app
  #   working_dir: /app
  #   command: python consumer_prediction.py
  #   restart: always

#   prediction_layer:
#     build:
#       context: .
#       dockerfile: ./prediction_layer/Dockerfile
#     container_name: prediction_layer
#     depends_on:
#       - kafka
#     environment:
#       KAFKA_BROKER: kafka:9092
#       KAFKA_TOPIC_AGGREGATED: aggregated_data
#       KAFKA_TOPIC_DASHBOARD: prediction
#       PYFLINK_CLASSPATH:  /opt/flink/lib/*:/opt/flink/opt/*  
#     mem_limit: 4g
#     volumes:
#       - ./create_model_lstm:/app/create_model_lstm
#     restart: unless-stopped
#     cpus: 1 




# ### DASHBOARD 

  # dashboard:
  #   build:
  #     context: ./dashboard
  #     dockerfile: Dockerfile
  #   container_name: dashboard
  #   ports:
  #     - "8501:8501"
  #   depends_on:
  #     - kafka
  #   environment:
  #     - PYTHONUNBUFFERED=1

# ## CLEANER MINIO

  # clear-minio:
  #   build:
  #     context: ./clean_minio
  #   environment:
  #     MINIO_URL: "minio:9000"
  #     MINIO_ACCESS_KEY: "admin"
  #     MINIO_SECRET_KEY: "admin123"
  #   command: ["python", "clean_minio.py"]


# HISTORICAL CONTAINER

# Historical producer

  producer_h_macrodata: # tutto sistemato
    build: ./historical_data/producer_h_macrodata
    environment:
      API_KEY_FRED: ${API_KEY_FRED}
    depends_on:
      - kafka


  producer_h_alpaca: # tutto sistemato
    build: ./historical_data/producer_h_alpaca
    environment:
      API_KEY_ALPACA: ${API_KEY_ALPACA}
      API_SECRET_ALPACA: ${API_SECRET_ALPACA}
      POSTGRES_HOST: ${POSTGRES_HOST} 
      POSTGRES_PORT: ${POSTGRES_PORT} 
      POSTGRES_DB: ${POSTGRES_DB}   
      POSTGRES_USER: ${POSTGRES_USER} 
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD} 
    depends_on:
      - kafka
      - postgres


  producer_h_company: # tutto sistemato
    build:
      context: ./historical_data/producer_h_company
    depends_on:
      - kafka


# historical consumer
  consumer_h_company: # tutto sistemato
    build:
      context: ./historical_data/consumer_h_company
    environment:
      S3_ENDPOINT_URL: ${S3_ENDPOINT_URL} 
      S3_ACCESS_KEY: ${S3_ACCESS_KEY} 
      S3_SECRET_KEY: ${S3_SECRET_KEY}
    depends_on:
      - kafka
      - minio
      - producer_h_company
         
  consumer_h_alpaca: # tutto sistemato
    build:
      context: ./historical_data/consumer_h_alpaca
    environment:
      S3_ENDPOINT_URL: ${S3_ENDPOINT_URL} 
      S3_ACCESS_KEY: ${S3_ACCESS_KEY} 
      S3_SECRET_KEY: ${S3_SECRET_KEY}
    depends_on:
      - kafka
      - minio
      - producer_h_alpaca

  consumer_h_macrodata:
    build:
      context: ./historical_data/consumer_h_macrodata
    environment:
      S3_ENDPOINT: ${S3_ENDPOINT} 
      S3_ACCESS_KEY: ${S3_ACCESS_KEY} 
      S3_SECRET_KEY: ${S3_SECRET_KEY}
      SPARK_SUBMIT_OPTIONS: --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0
    depends_on:
      - kafka
      - minio
      - producer_h_macrodata

  # flink_historical:
  #     build:
  #       context: ./historical_data/historical_aggregated
  #     restart: always
  #     environment:
  #       POSTGRES_HOST: ${POSTGRES_HOST} 
  #       POSTGRES_PORT: ${POSTGRES_PORT} 
  #       POSTGRES_DB: ${POSTGRES_DB}   
  #       POSTGRES_USER: ${POSTGRES_USER} 
  #       POSTGRES_PASSWORD: ${POSTGRES_PASSWORD} 
  #     depends_on:
  #       - kafka
  #       - postgres

networks:
  flink_net:
    driver: bridge

  minio_network:
    driver: bridge

volumes:
  pgdata:
    driver: local 