# Project Startup Guide

This project analyzes stock market trends in real time by combining financial data, macroeconomic indicators, and sentiment analysis from news and social media. The goal is to **provide a one-minute-ahead forecast of each company's stock price**, displayed on an interactive dashboard, which also allows real-time monitoring of market price anomalies.
This repository contains two Docker Compose configurations to launch the Stock Market Trend Analysis project in different modes: continuous streaming with a pre-trained model, and an option to download historical data and train the model from scratch.

### Prerequisites

To start and use the project, ensure you have **Docker** installed on your system. Furthermore, it is essential that Docker is configured with the following minimum resources:
- **RAM**: Minimum 8 GB RAM (allocated to Docker)
- **CPU**: Minimum 12 CPUs (allocated to Docker)

### Download of the .env file

Download the provided .env file and place it in the root directory of this repository. This file will contain necessary credentials and configuration settings.

### Alpaca Credentials

To use real stock market streaming and historical data, you need to configure your Alpaca credentials. You can obtain an **API_KEY_ALPACA** and an **API_SECRET_ALPACA** by registering through the Alpaca Trading API: https://alpaca.markets/. Alternatively, you can send an email to samuele.viola@studenti.unitn.it to receive updated credentials.

Once obtained, modify the environment variables in the .env file with your credentials (**API_KEY_ALPACA** and **API_SECRET_ALPACA**).

### Stock Market Data

The Alpaca (stock market) streaming data is real and is provided Monday to Friday from 9:30 AM to 4:00 PM (US Eastern Time, ET), which corresponds to **3:30 PM to 10:00 PM (Italian Summer Time, CEST)**. For the rest of the time, synthetic data will be generated to maintain the flow, and therefore, it is not necessary to have updated **API_KEY_ALPACA** and **API_SECRET_ALPACA**. The other data streams (macroeconomics data, company fundamentals, bluesky’s sentiment and news’ bluesky) are always real.

## 1. Docker Compose for Streaming (docker-compose-stream.yml)

This configuration is designed to start the real-time data stream and use an already trained model for prediction. It's the ideal option for those who want to see the project in action without having to manage the initial training.

#### Startup

To start only the streaming data flow and prediction, run the following command in your terminal:

<pre lang="markdown"> docker-compose -f docker-compose-stream.yml up --build -d </pre>

#### Dashboard Visualization

After launching the streaming configuration, you can access the dashboard at http://localhost:8501.
From the dropdown menu at the top, you can select a company and view both the stock price trend and the future prediction generated by the model, along with the real-time detection of potential market anomalies.

## 2. Docker Compose for Historical Data and Training (docker-compose-historical.yml)

This configuration allows you to download approximately 13 million rows of historical data (~4GB) and train the model from scratch. This is a longer process but gives you full control over the model. The first prediction will be available 5 minutes after the dashboard starts and will then continue in a continuous manner.

#### Startup

To download historical data and start model training, run the following command in your terminal:

<pre lang="markdown"> docker-compose -f docker-compose-historical.yml up --build -d </pre>

### Company Configuration

The project considers a specific set of companies. You can modify these companies by customizing the companies_info.csv file located in the postgres/ folder. If you want to change the companies to be considered in the project, it is **necessary** to restart docker-compose-historical.yml to re-download the historical data and retrain the models on the new companies. You can choose from all the companies listed on the New York Stock Exchange (NYSE) and NASDAQ.

Additionally, **you must remove the persistent PostgreSQL volume** that stores previous company data. Otherwise, the changes will not take effect.
Run the following command before restarting the configuration:

<pre lang="markdown"> docker volume rm bdt-project_pgdata </pre>

**Important**: With Alpaca's free API plan, you cannot exceed 30 companies.

In the companies_info.csv file:
- Modify the **"ticker_id"** column with a unique identification number.
- Modify the **"ticker"** column with the ticker of the company you wish to add.
- Modify the **"company_name"** column with the full company name (this name will appear in the dashboard and will be used for sentiment searches).
- Use the **"related_words"** column to add a second keyword to search for that company (e.g., a company nickname or a closely related term, like "Facebook" for "META").
- Set **"is_active"** to TRUE or FALSE to include or exclude a company from the project.

After modifying and saving the companies_info.csv file, you need to update the fundamental data for the companies. To do this, run the following command:

<pre lang="markdown"> docker exec -it app python company_fundamentals.py </pre>

**API Limits for Fundamental Data**: The API for fundamental data has a limit of 250 requests per day. Each company requires 3 API calls. Therefore, you cannot modify the full set of 30 companies more than 2 times on the same day.

### Managing Docker Compose

#### Shutting Down a Docker Compose

To shut down any Docker Compose configuration, use the command:

<pre lang="markdown"> docker-compose -f docker-compose-filename.yml down </pre>

For example, to shut down the historical configuration:

<pre lang="markdown"> docker-compose -f docker-compose-historical.yml down </pre>

#### Important Sequence for Starting Streams

When the model training with docker-compose-historical.yml is finished (you'll notice this from the container logs or when the training process stops), **you must shut down this configuration** (using the command docker-compose -f docker-compose-historical.yml down) before starting docker-compose-stream.yml.
