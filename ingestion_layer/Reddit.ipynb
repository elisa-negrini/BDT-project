{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "102fcbf7",
   "metadata": {},
   "source": [
    "## ----------------------- TOKEN -------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44936b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Configura le tue credenziali Reddit\n",
    "client_id = 'XpeJWmueFBEGkyXjE-dpcA'\n",
    "client_secret = 'yOyCLHaB0Ur7R0sW75UUmc20MjCPkw'\n",
    "user_agent = 'Samu_Miki'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f02944bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzione per ottenere un token\n",
    "def get_reddit_token(client_id, client_secret, user_agent):\n",
    "    url = 'https://www.reddit.com/api/v1/access_token'\n",
    "    headers = {\n",
    "        'User-Agent': user_agent\n",
    "    }\n",
    "    data = {\n",
    "        'grant_type': 'client_credentials'\n",
    "    }\n",
    "    auth = (client_id, client_secret)\n",
    "    \n",
    "    # Fai la richiesta per ottenere il token\n",
    "    response = requests.post(url, headers=headers, data=data, auth=auth)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        token = response.json()['access_token']\n",
    "        print(f'Token ottenuto con successo: {token}')\n",
    "        return token\n",
    "    else:\n",
    "        print(f'Errore durante l\\'ottenimento del token: {response.status_code}')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "837e953d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token ottenuto con successo: eyJhbGciOiJSUzI1NiIsImtpZCI6IlNIQTI1NjpzS3dsMnlsV0VtMjVmcXhwTU40cWY4MXE2OWFFdWFyMnpLMUdhVGxjdWNZIiwidHlwIjoiSldUIn0.eyJzdWIiOiJsb2lkIiwiZXhwIjoxNzQ2NjEyNDYxLjc4Mzc1NCwiaWF0IjoxNzQ2NTI2MDYxLjc4Mzc1NCwianRpIjoiZkhtOW5sVklsOWpULTNfclk3cHJDeDdmclRRRDJBIiwiY2lkIjoiWHBlSldtdWVGQkVHa3lYakUtZHBjQSIsImxpZCI6InQyXzFvc295Nzhob28iLCJsY2EiOjE3NDY1MjYwNjE3NjksInNjcCI6ImVKeUtWdEpTaWdVRUFBRF9fd056QVNjIiwiZmxvIjo2fQ.FEE6FFN7986HoZq3CCDqXoeRAXVXg6tHEgYIjRRop_iVVSVICBSPvAZbXZ0TP9Zf6vKxvDVwp5FDNlpkBjc5D-X49mzKxn2p86ku0xzenCgChvJgrAW9Muob6VzZQ-x7iXmj_siZe5B-Xq9LYexzDbvtpqjNV_XTLEkPiOVeAODlPdXZkK5d2wTldTBWfgU_BkS3wn2FskT7TivSVtjMHc3qk-JdHBG9TqXQ5Uv1rMMckLRrHoVQhjlwIqtFnKjwe2hYk-dSS03E6xKyCKAOsG7okCofEDcDGreoS9g5H8ytuBbxV0Uf9Gt1lhIGkvu2l0kZu68ff6VKThjcr4ykxw\n"
     ]
    }
   ],
   "source": [
    "#ottenere il token:\n",
    "token = get_reddit_token(client_id, client_secret, user_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338e4f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token ottenuto con successo: eyJhbGciOiJSUzI1NiIsImtpZCI6IlNIQTI1NjpzS3dsMnlsV0VtMjVmcXhwTU40cWY4MXE2OWFFdWFyMnpLMUdhVGxjdWNZIiwidHlwIjoiSldUIn0.eyJzdWIiOiJsb2lkIiwiZXhwIjoxNzQ2MzY2MDcwLjczMDcwMiwiaWF0IjoxNzQ2Mjc5NjcwLjczMDcwMiwianRpIjoibzV6NWlSSUVqZjZyZC1KU1JXSHNINnNSaXF4YXFnIiwiY2lkIjoiWHBlSldtdWVGQkVHa3lYakUtZHBjQSIsImxpZCI6InQyXzFvbG5yd2cyMmoiLCJsY2EiOjE3NDYyNzk2NzA3MTUsInNjcCI6ImVKeUtWdEpTaWdVRUFBRF9fd056QVNjIiwiZmxvIjo2fQ.ORd_uD01tYVBjNsnxYoB6niBK9gGO_ANwU0xDHBF82gMbOGf7ylOLmBfhuN_3i-spObT5ejoZwKRaDs7cqrSvDn4FXQPT6xpEDYEjp_kZLsvnVnlOqL7TuspDLXJHgyX8eVAt-BG3eDHY-cW90wAGz9Kbc16yFk5G7Hc-bMKyvOXqNMLKAhNtAqqNv_s1wrmrE2QavT1dMBTzSA0yK43gQmRJEp1B2dV83x5g3qi49vtdrzb3gSEurfRM0TNCDyAmOVrHOlZ2FjQUxexhHU1GMzXwE2xfvPv4uG5ssGSUumkymBGSSQcJA3mqxgeGQqm2GETnOv1pysiA144M-gHtQ\n",
      "Recuperati 100 post.\n",
      "Aspetta 24 ore per ottenere un nuovo token...\n"
     ]
    }
   ],
   "source": [
    "# PER ORA NON USARE, SERVE PER AVERNE UNO AL GIORNO, PER INTANTO LO FACCIAMO MANUALMENTE CON I COMANDI SOPRA\n",
    "\n",
    "# Ciclo per ottenere e usare il token ogni giorno\n",
    "while True:\n",
    "    # Ottieni il token (scade dopo 24 ore)\n",
    "    token = get_reddit_token(client_id, client_secret, user_agent)\n",
    "    \n",
    "    if token:\n",
    "        # Usa il token per fare la richiesta all'API\n",
    "        get_reddit_posts(token)\n",
    "    \n",
    "    # Aspetta 24 ore (86400 secondi) prima di ottenere un nuovo token\n",
    "    print('Aspetta 24 ore per ottenere un nuovo token...')\n",
    "    time.sleep(86400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f55b34",
   "metadata": {},
   "source": [
    "### --------------------- PROVIAMO CON PIU' SUBREDDIT --------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d157b5ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÅ Ciclo 1...\n",
      "üíæ Salvati 500 post MARKET nuovi\n",
      "üíæ Salvati 500 post GEOPOLITICS nuovi\n",
      "‚è≥ Pausa 30 secondi...\n",
      "\n",
      "\n",
      "üîÅ Ciclo 2...\n",
      "üíæ Salvati 1 post MARKET nuovi\n",
      "üíæ Salvati 1 post GEOPOLITICS nuovi\n",
      "‚è≥ Pausa 30 secondi...\n",
      "\n",
      "\n",
      "üîÅ Ciclo 3...\n",
      "üíæ Salvati 1 post MARKET nuovi\n",
      "‚è≥ Pausa 30 secondi...\n",
      "\n",
      "\n",
      "üîÅ Ciclo 4...\n",
      "‚è≥ Pausa 30 secondi...\n",
      "\n",
      "\n",
      "üîÅ Ciclo 5...\n",
      "‚è≥ Pausa 30 secondi...\n",
      "\n",
      "\n",
      "üîÅ Ciclo 6...\n",
      "‚è≥ Pausa 30 secondi...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "# Parametri\n",
    "market = [\"stocks\", \"investment\", \"wallstreetbets\", \"StockMarket\", \"financialindependence\"]\n",
    "geopolitics = [\"geopolitics\", \"politics\", \"worldnews\", \"news\", \"Economics\"]\n",
    "seen_ids = set()\n",
    "\n",
    "# Directory per salvare i parquet\n",
    "os.makedirs(\"data_reddit/market\", exist_ok=True)\n",
    "os.makedirs(\"data_reddit/geopolitics\", exist_ok=True)\n",
    "\n",
    "def get_new_reddit_posts(subreddit, token):\n",
    "    url = f'https://oauth.reddit.com/r/{subreddit}/new?limit=100'\n",
    "    headers = {\n",
    "        'Authorization': f'bearer {token}',\n",
    "        'User-Agent': user_agent\n",
    "    }\n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        posts = response.json()['data']['children']\n",
    "        posts_list = []\n",
    "\n",
    "        for post in posts:\n",
    "            post_data = post['data']\n",
    "            post_id = post_data.get('id')\n",
    "            if post_id not in seen_ids:\n",
    "                seen_ids.add(post_id)\n",
    "                posts_list.append({\n",
    "                    'id': post_id,\n",
    "                    'title': post_data.get('title'),\n",
    "                    'author': post_data.get('author'),\n",
    "                    'text': post_data.get('selftext', ''),\n",
    "                    'score': post_data.get('score'),\n",
    "                    'num_comments': post_data.get('num_comments'),\n",
    "                    'created_utc': post_data.get('created_utc'),\n",
    "                    'permalink': post_data.get('permalink'),\n",
    "                    'url': post_data.get('url'),\n",
    "                    'subreddit': post_data.get('subreddit')\n",
    "                })\n",
    "\n",
    "        df_new = pd.DataFrame(posts_list)\n",
    "        if not df_new.empty:\n",
    "            df_new['created_datetime'] = pd.to_datetime(df_new['created_utc'], unit='s')\n",
    "        return df_new\n",
    "    else:\n",
    "        print(f\"[{subreddit}] Errore nella richiesta: {response.status_code}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Cicli di polling\n",
    "for i in range(6):  # ad es. per 3 minuti (6 x 30s)\n",
    "    print(f\"\\nüîÅ Ciclo {i+1}...\")\n",
    "\n",
    "    timestamp_str = datetime.now(timezone.utc).strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    all_market_new = pd.DataFrame()\n",
    "    all_geo_new = pd.DataFrame()\n",
    "\n",
    "    for sub in market:\n",
    "        df_market = get_new_reddit_posts(sub, token)\n",
    "        all_market_new = pd.concat([all_market_new, df_market], ignore_index=True)\n",
    "    \n",
    "    for sub in geopolitics:\n",
    "        df_geo = get_new_reddit_posts(sub, token)\n",
    "        all_geo_new = pd.concat([all_geo_new, df_geo], ignore_index=True)\n",
    "\n",
    "    if not all_market_new.empty:\n",
    "        all_market_new.drop_duplicates(subset='id', inplace=True)\n",
    "        all_market_new.to_parquet(f\"data_reddit/market/market_{timestamp_str}.parquet\", index=False)\n",
    "        print(f\"üíæ Salvati {len(all_market_new)} post MARKET nuovi\")\n",
    "\n",
    "    if not all_geo_new.empty:\n",
    "        all_geo_new.drop_duplicates(subset='id', inplace=True)\n",
    "        all_geo_new.to_parquet(f\"data_reddit/geopolitics/geo_{timestamp_str}.parquet\", index=False)\n",
    "        print(f\"üíæ Salvati {len(all_geo_new)} post GEOPOLITICS nuovi\")\n",
    "\n",
    "    print(\"‚è≥ Pausa 30 secondi...\\n\")\n",
    "    time.sleep(30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab250af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>score</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>permalink</th>\n",
       "      <th>url</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>created_datetime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1kfh8qg</td>\n",
       "      <td>Pennsylvania House Health Committee Passes Mar...</td>\n",
       "      <td>OhMyOhWhyOh</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.746466e+09</td>\n",
       "      <td>/r/Economics/comments/1kfh8qg/pennsylvania_hou...</td>\n",
       "      <td>https://themarijuanaherald.com/2025/05/pennsyl...</td>\n",
       "      <td>Economics</td>\n",
       "      <td>2025-05-05 17:19:15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                              title       author  \\\n",
       "0  1kfh8qg  Pennsylvania House Health Committee Passes Mar...  OhMyOhWhyOh   \n",
       "\n",
       "  text  score  num_comments   created_utc  \\\n",
       "0           1             1  1.746466e+09   \n",
       "\n",
       "                                           permalink  \\\n",
       "0  /r/Economics/comments/1kfh8qg/pennsylvania_hou...   \n",
       "\n",
       "                                                 url  subreddit  \\\n",
       "0  https://themarijuanaherald.com/2025/05/pennsyl...  Economics   \n",
       "\n",
       "     created_datetime  \n",
       "0 2025-05-05 17:19:15  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "from datetime import datetime, timezone\n",
    "from kafka import KafkaProducer\n",
    "import json\n",
    "\n",
    "# Config\n",
    "market = [\"stocks\", \"investment\", \"wallstreetbets\", \"StockMarket\", \"financialindependence\"]\n",
    "geopolitics = [\"geopolitics\", \"politics\", \"worldnews\", \"news\", \"Economics\"]\n",
    "seen_ids = set()\n",
    "\n",
    "# Kafka Producer\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers='localhost:9092',\n",
    "    value_serializer=lambda v: json.dumps(v).encode('utf-8')\n",
    ")\n",
    "\n",
    "def get_new_reddit_posts(subreddit, token):\n",
    "    url = f'https://oauth.reddit.com/r/{subreddit}/new?limit=100'\n",
    "    headers = {\n",
    "        'Authorization': f'bearer {token}',\n",
    "        'User-Agent': user_agent\n",
    "    }\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        posts = response.json()['data']['children']\n",
    "        new_posts = []\n",
    "\n",
    "        for post in posts:\n",
    "            post_data = post['data']\n",
    "            post_id = post_data.get('id')\n",
    "            if post_id not in seen_ids:\n",
    "                seen_ids.add(post_id)\n",
    "                new_posts.append({\n",
    "                    'id': post_id,\n",
    "                    'title': post_data.get('title'),\n",
    "                    'author': post_data.get('author'),\n",
    "                    'text': post_data.get('selftext', ''),\n",
    "                    'score': post_data.get('score'),\n",
    "                    'num_comments': post_data.get('num_comments'),\n",
    "                    'created_utc': post_data.get('created_utc'),\n",
    "                    'permalink': post_data.get('permalink'),\n",
    "                    'url': post_data.get('url'),\n",
    "                    'subreddit': post_data.get('subreddit')\n",
    "                })\n",
    "        return new_posts\n",
    "    else:\n",
    "        print(f\"[{subreddit}] Errore nella richiesta: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "# Polling loop\n",
    "for i in range(6):  # ad esempio per 3 minuti\n",
    "    print(f\"\\nüîÅ Ciclo {i+1}...\")\n",
    "\n",
    "    for sub in market:\n",
    "        posts = get_new_reddit_posts(sub, token)\n",
    "        for post in posts:\n",
    "            producer.send(\"reddit_market\", value=post)\n",
    "        print(f\"üì§ Inviati {len(posts)} post MARKET da /r/{sub}\")\n",
    "\n",
    "    for sub in geopolitics:\n",
    "        posts = get_new_reddit_posts(sub, token)\n",
    "        for post in posts:\n",
    "            producer.send(\"reddit_geopolitics\", value=post)\n",
    "        print(f\"üì§ Inviati {len(posts)} post GEO da /r/{sub}\")\n",
    "\n",
    "    print(\"‚è≥ Pausa 30 secondi...\\n\")\n",
    "    time.sleep(30)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c07f741",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaConsumer\n",
    "\n",
    "consumer = KafkaConsumer(\n",
    "    'reddit_market',\n",
    "    bootstrap_servers='localhost:9092',\n",
    "    value_deserializer=lambda x: json.loads(x.decode('utf-8')),\n",
    "    auto_offset_reset='earliest',\n",
    "    group_id='reddit-test-group'\n",
    ")\n",
    "\n",
    "for msg in consumer:\n",
    "    print(msg.value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acaeef81",
   "metadata": {},
   "source": [
    "## --------------------- With Comments -----------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631ce177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÅ Ciclo 1...\n",
      "[stocks] Post trovati: 10\n",
      "Nuovi aggiunti: 10\n",
      "[investment] Post trovati: 10\n",
      "Nuovi aggiunti: 10\n",
      "[wallstreetbets] Post trovati: 10\n",
      "Nuovi aggiunti: 10\n",
      "[StockMarket] Post trovati: 10\n",
      "Nuovi aggiunti: 10\n",
      "[financialindependence] Post trovati: 10\n",
      "Nuovi aggiunti: 10\n",
      "[geopolitics] Post trovati: 10\n",
      "Nuovi aggiunti: 10\n",
      "[politics] Post trovati: 10\n",
      "Nuovi aggiunti: 10\n",
      "[worldnews] Post trovati: 10\n",
      "Nuovi aggiunti: 10\n",
      "[news] Post trovati: 10\n",
      "Nuovi aggiunti: 10\n",
      "[Economics] Post trovati: 10\n",
      "Nuovi aggiunti: 10\n",
      "‚è≥ Pausa 30 secondi...\n",
      "\n",
      "\n",
      "üîÅ Ciclo 2...\n",
      "[stocks] Post trovati: 10\n",
      "Nuovi aggiunti: 1\n",
      "[investment] Post trovati: 10\n",
      "[wallstreetbets] Post trovati: 10\n",
      "Nuovi aggiunti: 2\n",
      "[StockMarket] Post trovati: 10\n",
      "[financialindependence] Post trovati: 10\n",
      "[geopolitics] Post trovati: 10\n",
      "[politics] Post trovati: 10\n",
      "Nuovi aggiunti: 1\n",
      "[worldnews] Post trovati: 10\n",
      "Nuovi aggiunti: 3\n",
      "[news] Post trovati: 10\n",
      "[Economics] Post trovati: 10\n",
      "Nuovi aggiunti: 1\n",
      "‚è≥ Pausa 30 secondi...\n",
      "\n",
      "\n",
      "‚úÖ Totale post MARKET raccolti: 50\n",
      "‚úÖ Totale post GEOPOLITICS raccolti: 50\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "# Liste di subreddit\n",
    "market = [\"stocks\", \"investment\", \"wallstreetbets\", \"StockMarket\", \"financialindependence\"]\n",
    "geopolitics = [\"geopolitics\", \"politics\", \"worldnews\", \"news\", \"Economics\"]\n",
    "\n",
    "\n",
    "# Inizializza i dataframe e l'insieme per evitare duplicati\n",
    "all_market_posts = pd.DataFrame()\n",
    "all_geo_posts = pd.DataFrame()\n",
    "seen_ids = set()\n",
    "post_comment_tracker = {}\n",
    "\n",
    "# Funzione per scaricare i commenti di un post\n",
    "def get_comments(post_id):\n",
    "    url = f'https://oauth.reddit.com/comments/{post_id}?limit=100'\n",
    "    headers = {\n",
    "        'Authorization': f'bearer {token}',\n",
    "        'User-Agent': user_agent\n",
    "    }\n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        comments_data = response.json()[1]['data']['children']\n",
    "        comments_list = []\n",
    "        for comment in comments_data:\n",
    "            comment_data = comment['data']\n",
    "            comments_list.append({\n",
    "                'comment_id': comment_data.get('id'),\n",
    "                'author': comment_data.get('author'),\n",
    "                'score': comment_data.get('score'),\n",
    "                'body': comment_data.get('body'),\n",
    "                'created_utc': comment_data.get('created_utc'),\n",
    "                'post_id': post_id\n",
    "            })\n",
    "        return pd.DataFrame(comments_list)\n",
    "    else:\n",
    "        print(f\"Errore nel recuperare i commenti per il post {post_id}: {response.status_code}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Funzione per scaricare post da un subreddit e i loro commenti\n",
    "def get_new_reddit_posts(subreddit, token):\n",
    "\n",
    "    url = f'https://oauth.reddit.com/r/{subreddit}/new?limit=10'\n",
    "    headers = {\n",
    "        'Authorization': f'bearer {token}',\n",
    "        'User-Agent': user_agent\n",
    "    }\n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        posts = data['data']['children']\n",
    "        posts_list = []\n",
    "\n",
    "        for post in posts:\n",
    "            post_data = post['data']\n",
    "            post_id = post_data.get('id')\n",
    "            current_num_comments = post_data.get(\"num_comments\", 0)\n",
    "\n",
    "            should_fetch_comments = False\n",
    "\n",
    "            if post_id not in seen_ids:\n",
    "                seen_ids.add(post_id)\n",
    "                should_fetch_comments = True\n",
    "            elif post_comment_tracker.get(post_id) != current_num_comments:\n",
    "                should_fetch_comments = True\n",
    "\n",
    "            if should_fetch_comments:\n",
    "                post_comment_tracker[post_id] = current_num_comments\n",
    "                # Recupera anche i commenti per ogni post\n",
    "                comments_df = get_comments(post_id)\n",
    "                \n",
    "                # Aggiungi i post e i commenti a un unico dataframe\n",
    "                posts_list.append({\n",
    "                    'id': post_id,\n",
    "                    'title': post_data.get('title'),\n",
    "                    'text': post_data.get('selftext', ''),\n",
    "                    'author': post_data.get('author'),\n",
    "                    'score': post_data.get('score'),\n",
    "                    'num_comments': post_data.get('num_comments'),\n",
    "                    'created_utc': post_data.get('created_utc'),\n",
    "                    'permalink': post_data.get('permalink'),\n",
    "                    'url': post_data.get('url'),\n",
    "                    'subreddit': post_data.get('subreddit'),\n",
    "                    'comments': comments_df  # Aggiungi il dataframe dei commenti\n",
    "                })\n",
    "                \n",
    "        df_new = pd.DataFrame(posts_list)\n",
    "        print(f\"[{subreddit}] Post trovati: {len(posts)}\")\n",
    "        if not df_new.empty:\n",
    "            df_new['created_datetime'] = pd.to_datetime(df_new['created_utc'], unit='s')\n",
    "            print(f\"Nuovi aggiunti: {len(df_new)}\")\n",
    "        return df_new\n",
    "    else:\n",
    "        print(f\"[{subreddit}] Errore nella richiesta: {response.status_code}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Esegui il polling ogni 30 secondi per N cicli\n",
    "for i in range(6):  # esegue per 3 minuti (6 x 30s)\n",
    "    print(f\"\\nüîÅ Ciclo {i+1}...\")\n",
    "    \n",
    "    # Post dai subreddit di mercato\n",
    "    for sub in market:\n",
    "        df_market = get_new_reddit_posts(sub, token)\n",
    "        all_market_posts = pd.concat([all_market_posts, df_market], ignore_index=True)\n",
    "    \n",
    "    # Post dai subreddit geopolitici\n",
    "    for sub in geopolitics:\n",
    "        df_geo = get_new_reddit_posts(sub, token)\n",
    "        all_geo_posts = pd.concat([all_geo_posts, df_geo], ignore_index=True)\n",
    "\n",
    "    print(\"‚è≥ Pausa 30 secondi...\\n\")\n",
    "    time.sleep(30)\n",
    "\n",
    "# Rimuovi duplicati\n",
    "all_market_posts.drop_duplicates(subset='id', inplace=True)\n",
    "all_geo_posts.drop_duplicates(subset='id', inplace=True)\n",
    "\n",
    "print(f\"\\n‚úÖ Totale post MARKET raccolti: {len(all_market_posts)}\")\n",
    "print(f\"‚úÖ Totale post GEOPOLITICS raccolti: {len(all_geo_posts)}\")\n",
    "\n",
    "# Salvataggio (opzionale)\n",
    "# all_market_posts.to_csv(\"market_posts.csv\", index=False)\n",
    "# all_geo_posts.to_csv(\"geo_posts.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec04e411",
   "metadata": {},
   "source": [
    "Per intanto accantoniamo lo script con i commenti perch√® troppo difficile se devono essere aggiornati, tuttavia, se volessimo andare avanti, risolvere il problema del drop duplicates, perch√® per come √® messo ora, elimina il duplicato pi√π aggiornato, invece dovrebbe tenere quello pi√π aggiornato"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "big-data-finance",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
