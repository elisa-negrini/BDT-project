{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6bfd9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Configura le tue credenziali Reddit\n",
    "client_id = 'XpeJWmueFBEGkyXjE-dpcA'\n",
    "client_secret = 'yOyCLHaB0Ur7R0sW75UUmc20MjCPkw'\n",
    "user_agent = 'Samu_Miki'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69956c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzione per ottenere un token\n",
    "def get_reddit_token(client_id, client_secret, user_agent):\n",
    "    url = 'https://www.reddit.com/api/v1/access_token'\n",
    "    headers = {\n",
    "        'User-Agent': user_agent\n",
    "    }\n",
    "    data = {\n",
    "        'grant_type': 'client_credentials'\n",
    "    }\n",
    "    auth = (client_id, client_secret)\n",
    "    \n",
    "    # Fai la richiesta per ottenere il token\n",
    "    response = requests.post(url, headers=headers, data=data, auth=auth)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        token = response.json()['access_token']\n",
    "        print(f'Token ottenuto con successo: {token}')\n",
    "        return token\n",
    "    else:\n",
    "        print(f'Errore durante l\\'ottenimento del token: {response.status_code}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dec19b0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token ottenuto con successo: eyJhbGciOiJSUzI1NiIsImtpZCI6IlNIQTI1NjpzS3dsMnlsV0VtMjVmcXhwTU40cWY4MXE2OWFFdWFyMnpLMUdhVGxjdWNZIiwidHlwIjoiSldUIn0.eyJzdWIiOiJsb2lkIiwiZXhwIjoxNzQ2NjI1NjI3LjcwNDk1NSwiaWF0IjoxNzQ2NTM5MjI3LjcwNDk1NSwianRpIjoiUy1CVnNrY3Z6aHgxUVhkcEhIS1JRd2NPaTYwOWtBIiwiY2lkIjoiWHBlSldtdWVGQkVHa3lYakUtZHBjQSIsImxpZCI6InQyXzFvdDJpZHdrMW4iLCJsY2EiOjE3NDY1MzkyMjc2ODgsInNjcCI6ImVKeUtWdEpTaWdVRUFBRF9fd056QVNjIiwiZmxvIjo2fQ.fnmMbx26JiULeFNa03birYd5KqlR8SVsxDFpMeaErnIEnQMRbU0S0iS6xkJ1hxZIOo8qN1piTGYNkTwiht6MlOi2pmHFRKhYHjyVxFt00eInjHIyhaX552ou7koJgPqvmfKcMdvexRwLEeg4VnEqEVYNf73zxRaeGa9pXj601l140Qa0K_wpPnTT48BO6qM-wcJL77jxtpW8hnYxcCO0UJ6_PUy-7iVka2NW6cMmrkZK9zhAzLkbABY_7qdnL0AqSuP8MR85nahS-Qvs7dm8eAR2BmorASSEEncS8XtJmoyezOEZkzicRTJhrzL-Bry2RclM38PzVModi5g1bl0qmg\n"
     ]
    }
   ],
   "source": [
    "#ottenere il token:\n",
    "token = get_reddit_token(client_id, client_secret, user_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28d519e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PER ORA NON USARE, SERVE PER AVERNE UNO AL GIORNO, PER INTANTO LO FACCIAMO MANUALMENTE CON I COMANDI SOPRA\n",
    "\n",
    "# Ciclo per ottenere e usare il token ogni giorno\n",
    "while True:\n",
    "    # Ottieni il token (scade dopo 24 ore)\n",
    "    token = get_reddit_token(client_id, client_secret, user_agent)\n",
    "    \n",
    "    if token:\n",
    "        # Usa il token per fare la richiesta all'API\n",
    "        get_reddit_posts(token)\n",
    "    \n",
    "    # Aspetta 24 ore (86400 secondi) prima di ottenere un nuovo token\n",
    "    print('Aspetta 24 ore per ottenere un nuovo token...')\n",
    "    time.sleep(86400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cb10bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRODUCER REDDIT\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "from datetime import datetime, timezone\n",
    "from kafka import KafkaProducer\n",
    "import json\n",
    "\n",
    "# Config\n",
    "market = [\"stocks\", \"investment\", \"wallstreetbets\", \"StockMarket\", \"financialindependence\"]\n",
    "geopolitics = [\"geopolitics\", \"politics\", \"worldnews\", \"news\", \"Economics\"]\n",
    "seen_ids = set()\n",
    "token = \"eyJhbGciOiJSUzI1NiIsImtpZCI6IlNIQTI1NjpzS3dsMnlsV0VtMjVmcXhwTU40cWY4MXE2OWFFdWFyMnpLMUdhVGxjdWNZIiwidHlwIjoiSldUIn0.eyJzdWIiOiJsb2lkIiwiZXhwIjoxNzQ2NjI1NjI3LjcwNDk1NSwiaWF0IjoxNzQ2NTM5MjI3LjcwNDk1NSwianRpIjoiUy1CVnNrY3Z6aHgxUVhkcEhIS1JRd2NPaTYwOWtBIiwiY2lkIjoiWHBlSldtdWVGQkVHa3lYakUtZHBjQSIsImxpZCI6InQyXzFvdDJpZHdrMW4iLCJsY2EiOjE3NDY1MzkyMjc2ODgsInNjcCI6ImVKeUtWdEpTaWdVRUFBRF9fd056QVNjIiwiZmxvIjo2fQ.fnmMbx26JiULeFNa03birYd5KqlR8SVsxDFpMeaErnIEnQMRbU0S0iS6xkJ1hxZIOo8qN1piTGYNkTwiht6MlOi2pmHFRKhYHjyVxFt00eInjHIyhaX552ou7koJgPqvmfKcMdvexRwLEeg4VnEqEVYNf73zxRaeGa9pXj601l140Qa0K_wpPnTT48BO6qM-wcJL77jxtpW8hnYxcCO0UJ6_PUy-7iVka2NW6cMmrkZK9zhAzLkbABY_7qdnL0AqSuP8MR85nahS-Qvs7dm8eAR2BmorASSEEncS8XtJmoyezOEZkzicRTJhrzL-Bry2RclM38PzVModi5g1bl0qmg\"\n",
    "client_id = 'XpeJWmueFBEGkyXjE-dpcA'\n",
    "client_secret = 'yOyCLHaB0Ur7R0sW75UUmc20MjCPkw'\n",
    "user_agent = 'Samu_Miki'\n",
    "\n",
    "\n",
    "# Kafka Producer\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers='kafka:9092',\n",
    "    value_serializer=lambda v: json.dumps(v).encode('utf-8')\n",
    ")\n",
    "\n",
    "def get_new_reddit_posts(subreddit, token):\n",
    "    url = f'https://oauth.reddit.com/r/{subreddit}/new?limit=100'\n",
    "    headers = {\n",
    "        'Authorization': f'bearer {token}',\n",
    "        'User-Agent': user_agent\n",
    "    }\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        posts = response.json()['data']['children']\n",
    "        new_posts = []\n",
    "\n",
    "        for post in posts:\n",
    "            post_data = post['data']\n",
    "            post_id = post_data.get('id')\n",
    "            if post_id not in seen_ids:\n",
    "                seen_ids.add(post_id)\n",
    "                new_posts.append({\n",
    "                    'id': post_id,\n",
    "                    'title': post_data.get('title'),\n",
    "                    'author': post_data.get('author'),\n",
    "                    'text': post_data.get('selftext', ''),\n",
    "                    'score': post_data.get('score'),\n",
    "                    'num_comments': post_data.get('num_comments'),\n",
    "                    'created_utc': post_data.get('created_utc'),\n",
    "                    'permalink': post_data.get('permalink'),\n",
    "                    'url': post_data.get('url'),\n",
    "                    'subreddit': post_data.get('subreddit')\n",
    "                })\n",
    "        return new_posts\n",
    "    else:\n",
    "        print(f\"[{subreddit}] Errore nella richiesta: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "topic = \"test_topic\"\n",
    "# Polling loop\n",
    "for i in range(6):  # ad esempio per 3 minuti\n",
    "    print(f\"\\nüîÅ Ciclo {i+1}...\")\n",
    "\n",
    "    for sub in market:\n",
    "        posts = get_new_reddit_posts(sub, token)\n",
    "        for post in posts:\n",
    "            producer.send(topic, value=post)\n",
    "        print(f\"üì§ Inviati {len(posts)} post MARKET da /r/{sub}\")\n",
    "\n",
    "    for sub in geopolitics:\n",
    "        posts = get_new_reddit_posts(sub, token)\n",
    "        for post in posts:\n",
    "            producer.send(topic, value=post)\n",
    "        print(f\"üì§ Inviati {len(posts)} post GEO da /r/{sub}\")\n",
    "\n",
    "    print(\"‚è≥ Pausa 30 secondi...\\n\")\n",
    "    time.sleep(30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a59a016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONSUMER REDDIT \n",
    "\n",
    "from kafka import KafkaConsumer\n",
    "import json\n",
    "import os\n",
    "\n",
    "topic = \"test_topic\"\n",
    "kafka_server = os.getenv(\"KAFKA_SERVER\", \"kafka:9092\")\n",
    "\n",
    "# Percorso del file dove salvare i dati (es: /data/output.jsonl nel container)\n",
    "output_path = \"/data/output.jsonl\"\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "consumer = KafkaConsumer(\n",
    "    topic,\n",
    "    bootstrap_servers=[kafka_server],\n",
    "    auto_offset_reset='latest',\n",
    "    value_deserializer=lambda x: json.loads(x.decode('utf-8'))\n",
    ")\n",
    "\n",
    "print(f\"üîå In ascolto sul topic '{topic}'... Salvando in {output_path}\")\n",
    "\n",
    "with open(output_path, \"a\", encoding=\"utf-8\") as f:\n",
    "    for message in consumer:\n",
    "        data = message.value\n",
    "        print(\"üìù Ricevuto:\", data['title'][:80])\n",
    "        json.dump(data, f, ensure_ascii=False)\n",
    "        f.write(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "big-data-finance",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
